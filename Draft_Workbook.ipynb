{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, you will need to CMD and authenticate with \n",
    "\n",
    "'gcloud auth application-default login'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project='manifest-frame-203601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"\"\"\n",
    "    select * from w266_final.final_20k\n",
    "    LIMIT 10000\"\"\")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "df = []\n",
    "for row in rows:\n",
    "    df.append([row.repo_path,row.c_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watchdogpolska/feder feder/records/types.py</td>\n",
       "      <td>from abc import abstractmethod, ABCMeta\\n\\n\\nc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>softappeal/yass py2/test/contract_test.py</td>\n",
       "      <td>import unittest\\nfrom typing import Any\\n\\nimp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gcarq/freqtrade freqtrade/tests/test_fiat_conv...</td>\n",
       "      <td># pragma pylint: disable=missing-docstring, to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>devilry/devilry-django devilry/devilry_compres...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Generated by Django...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>erigones/esdc-ce api/dc/storage/serializers.py</td>\n",
       "      <td>from api import serializers as s\\nfrom vms.mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>moddevices/mod-ui mod/settings.py</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n\\n# Copyright 2012-20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ms-iot/python cpython/Tools/unicode/gencodec.py</td>\n",
       "      <td>\"\"\" Unicode Mapping Parser and Codec Generator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flammified/terrabot terrabot/packets/packet39.py</td>\n",
       "      <td>import struct\\n\\n\\nclass Packet39Parser(object...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lukasmonk/lucaschess Code/QT/PantallaConfig.py</td>\n",
       "      <td>from PyQt4 import QtCore\\n\\nfrom Code import D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MetaMetricsInc/django-static-version example/e...</td>\n",
       "      <td>\"\"\"\\nDjango settings for example project.\\n\\nG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>maas/maas setup.py</td>\n",
       "      <td># Copyright 2012-2016 Canonical Ltd.  This sof...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>enigmagroup/enigmabox-openwrt webinterface/fil...</td>\n",
       "      <td>\"\"\"\\nWSGI config for cnc project.\\n\\nThis modu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cfce/chilin chilin2/modules/ceas/dc.py</td>\n",
       "      <td>\"\"\"\\nDC version of ceas\\n\"\"\"\\nfrom samflow.com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>intel-iot-devkit/upm examples/python/loudness.py</td>\n",
       "      <td>#!/usr/bin/env python\\n# Author: Jon Trulson &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>mattbernst/polyhartree support/ansible/modules...</td>\n",
       "      <td>#!/usr/bin/python\\n\\nDOCUMENTATION = \"\"\"\\nmodu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kulbear/deep-learning-nano-foundation DLND-tv-...</td>\n",
       "      <td>import numpy as np\\nimport tensorflow as tf\\nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Hackerfleet/hfos modules/countables/hfos/count...</td>\n",
       "      <td>#!/usr/bin/env python\\n# -*- coding: UTF-8 -*-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>iBluemind/armatis armatis/parsers/epost.py</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n\\nfrom armatis.models...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bzero/bitex libs/autobahn/tests/test_wamp.py</td>\n",
       "      <td>##############################################...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ohmu/pglookout test/test_cluster_monitor.py</td>\n",
       "      <td>\"\"\"\\npglookout\\n\\nCopyright (c) 2015 Ohmu Ltd\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>openstack/neutron-fwaas neutron_fwaas/tests/fu...</td>\n",
       "      <td># Copyright 2015 Red Hat, Inc.\\n#\\n#    Licens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Southpaw-TACTIC/TACTIC src/pyasm/security/auth...</td>\n",
       "      <td>##############################################...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ellmetha/django-machina machina/templatetags/f...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n\\nfrom __future__ imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>exercism/python exercises/hello-world/example.py</td>\n",
       "      <td>def hello():\\n    return 'Hello, World!'\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>memsql/memsql-python memsql/common/util.py</td>\n",
       "      <td>def timedelta_total_seconds(td):\\n    \"\"\" Need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>general-language-syntax/GLS test/integration/F...</td>\n",
       "      <td>#\\ndef aaa():\\n    # ...\\n#\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>miguelgrinberg/python-socketio socketio/namesp...</td>\n",
       "      <td>class Namespace(object):\\n    \"\"\"Base class fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>practicalswift/bitcoin share/rpcauth/rpcauth.py</td>\n",
       "      <td>#!/usr/bin/env python3\\n# Copyright (c) 2015-2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>BillBarry/pyqwc pyqwc/clientlib.py</td>\n",
       "      <td>import walrus\\nimport uuid\\nfrom configobj imp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>eleweek/WatchPeopleCode migrations/versions/55...</td>\n",
       "      <td>\"\"\"empty message\\n\\nRevision ID: 552800b5e708\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9970</th>\n",
       "      <td>YeEmrick/learning PCI/PCI_Code/chapter5/dorm.py</td>\n",
       "      <td>import random\\r\\nimport math\\r\\n\\r\\n# The dorm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9971</th>\n",
       "      <td>joedicastro/python-recipes dir_size_monitor.py</td>\n",
       "      <td>#!/usr/bin/env python2\\n# -*- coding: utf8 -*-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9972</th>\n",
       "      <td>arnomoonens/DeepRL agents/actorcritic/paramete...</td>\n",
       "      <td>#!/usr/bin/env python\\n# -*- coding: utf8 -*-\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9973</th>\n",
       "      <td>openstack/rally tests/unit/task/test_types.py</td>\n",
       "      <td># Copyright (C) 2014 Yahoo! Inc. All Rights Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9974</th>\n",
       "      <td>KhronosGroup/COLLADA-CTS Core/Gui/FAnimation.py</td>\n",
       "      <td># Copyright (c) 2012 The Khronos Group Inc.\\n#...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9975</th>\n",
       "      <td>mishbahr/django-connected connected_accounts/p...</td>\n",
       "      <td>from django.utils.translation import ugettext_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9976</th>\n",
       "      <td>ElementalAlchemist/txircd txircd/modules/ircv3...</td>\n",
       "      <td>from twisted.plugin import IPlugin\\nfrom txirc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9977</th>\n",
       "      <td>pyupio/changelogs changelogs/custom/pypi/pytz.py</td>\n",
       "      <td>def get_urls(*args, **kwargs):\\n    return {'h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>cineuse/CNCGToolKit pyLibs/Qt.py</td>\n",
       "      <td>\"\"\"Map all bindings to PySide2\\n\\nThis module ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9979</th>\n",
       "      <td>pwollstadt/IDTxl test/test_idtxl_import.py</td>\n",
       "      <td>\"\"\"Provide unit tests for IDTxl's import funct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9980</th>\n",
       "      <td>NMGRL/pychron pychron/spectrometer/jobs/relati...</td>\n",
       "      <td># ============================================...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9981</th>\n",
       "      <td>austromorph/cartogram3 cartogram_dialog.py</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n\"\"\"\\n/***************...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9982</th>\n",
       "      <td>maxplanck-ie/HiCExplorer hicexplorer/parserCom...</td>\n",
       "      <td>import argparse\\nimport logging\\nlog = logging...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9983</th>\n",
       "      <td>clade/PyDAQmx PyDAQmxTest/test_misc.py</td>\n",
       "      <td>import unittest\\nimport numpy as np\\nimport ct...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9984</th>\n",
       "      <td>drepetto/chiplotle chiplotle/examples/abstract...</td>\n",
       "      <td>#!/usr/bin/env python\\n\\nfrom chiplotle import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9985</th>\n",
       "      <td>cloudera/kudu build-support/compile_flags.py</td>\n",
       "      <td>#!/usr/bin/env python\\n#\\n# Licensed to the Ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9986</th>\n",
       "      <td>MTG/essentia test/src/unittests/sfx/test_flatn...</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# Copyright (C) 2006-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9987</th>\n",
       "      <td>leiferikb/bitpop build/third_party/twisted_10_...</td>\n",
       "      <td># -*- test-case-name: twisted.words.test.test_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9988</th>\n",
       "      <td>xbmc/atv2 xbmc/lib/libPython/Python/Lib/compil...</td>\n",
       "      <td>\"\"\"Parser for future statements\\n\\n\"\"\"\\n\\nfrom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9989</th>\n",
       "      <td>XvBMC/repository.xvbmc Dependencies/script.mod...</td>\n",
       "      <td>import re,time\\nimport requests,xbmcaddon\\nfro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>eve-val/evelink tests/test_map.py</td>\n",
       "      <td>import mock\\n\\nfrom tests.compat import unitte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>mirumee/saleor saleor/dashboard/group/views.py</td>\n",
       "      <td>from django.conf import settings\\nfrom django....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>mollyproject/mollyproject molly/apps/feature_v...</td>\n",
       "      <td># encoding: utf-8\\nimport datetime\\nfrom south...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9993</th>\n",
       "      <td>grnet/e-science tests/web/test_destroy_cluster.py</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n'''\\nThis script is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>yosinski/OpenSCAD-playground tree.py</td>\n",
       "      <td>#! /usr/bin/python\\n\\n# Copyright 2012 by Jaso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>zhenxuan00/mmdgm conv-mmdgm/layer/NoParamsGaus...</td>\n",
       "      <td>import theano.tensor as T\\nimport theano.tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>cpbotha/devide modules/vtk_basic/vtkStructured...</td>\n",
       "      <td># class generated by DeVIDE::createDeVIDEModul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>swoodford/raspberrypi speedtest.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# http://www.pcworld....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>asciidisco/plugin.video.telekom-sport resource...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Module: Utils\\n# Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>XefPatterson/INF8225_Project ChatBox/GUI/ui_ch...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\n# Source : Werkov, Gi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              repo_path  \\\n",
       "0           watchdogpolska/feder feder/records/types.py   \n",
       "1             softappeal/yass py2/test/contract_test.py   \n",
       "2     gcarq/freqtrade freqtrade/tests/test_fiat_conv...   \n",
       "3     devilry/devilry-django devilry/devilry_compres...   \n",
       "4        erigones/esdc-ce api/dc/storage/serializers.py   \n",
       "5                     moddevices/mod-ui mod/settings.py   \n",
       "6       ms-iot/python cpython/Tools/unicode/gencodec.py   \n",
       "7      flammified/terrabot terrabot/packets/packet39.py   \n",
       "8        lukasmonk/lucaschess Code/QT/PantallaConfig.py   \n",
       "9     MetaMetricsInc/django-static-version example/e...   \n",
       "10                                   maas/maas setup.py   \n",
       "11    enigmagroup/enigmabox-openwrt webinterface/fil...   \n",
       "12               cfce/chilin chilin2/modules/ceas/dc.py   \n",
       "13     intel-iot-devkit/upm examples/python/loudness.py   \n",
       "14    mattbernst/polyhartree support/ansible/modules...   \n",
       "15    Kulbear/deep-learning-nano-foundation DLND-tv-...   \n",
       "16    Hackerfleet/hfos modules/countables/hfos/count...   \n",
       "17           iBluemind/armatis armatis/parsers/epost.py   \n",
       "18         bzero/bitex libs/autobahn/tests/test_wamp.py   \n",
       "19          ohmu/pglookout test/test_cluster_monitor.py   \n",
       "20    openstack/neutron-fwaas neutron_fwaas/tests/fu...   \n",
       "21    Southpaw-TACTIC/TACTIC src/pyasm/security/auth...   \n",
       "22    ellmetha/django-machina machina/templatetags/f...   \n",
       "23     exercism/python exercises/hello-world/example.py   \n",
       "24           memsql/memsql-python memsql/common/util.py   \n",
       "25    general-language-syntax/GLS test/integration/F...   \n",
       "26    miguelgrinberg/python-socketio socketio/namesp...   \n",
       "27      practicalswift/bitcoin share/rpcauth/rpcauth.py   \n",
       "28                   BillBarry/pyqwc pyqwc/clientlib.py   \n",
       "29    eleweek/WatchPeopleCode migrations/versions/55...   \n",
       "...                                                 ...   \n",
       "9970    YeEmrick/learning PCI/PCI_Code/chapter5/dorm.py   \n",
       "9971     joedicastro/python-recipes dir_size_monitor.py   \n",
       "9972  arnomoonens/DeepRL agents/actorcritic/paramete...   \n",
       "9973      openstack/rally tests/unit/task/test_types.py   \n",
       "9974    KhronosGroup/COLLADA-CTS Core/Gui/FAnimation.py   \n",
       "9975  mishbahr/django-connected connected_accounts/p...   \n",
       "9976  ElementalAlchemist/txircd txircd/modules/ircv3...   \n",
       "9977   pyupio/changelogs changelogs/custom/pypi/pytz.py   \n",
       "9978                   cineuse/CNCGToolKit pyLibs/Qt.py   \n",
       "9979         pwollstadt/IDTxl test/test_idtxl_import.py   \n",
       "9980  NMGRL/pychron pychron/spectrometer/jobs/relati...   \n",
       "9981         austromorph/cartogram3 cartogram_dialog.py   \n",
       "9982  maxplanck-ie/HiCExplorer hicexplorer/parserCom...   \n",
       "9983             clade/PyDAQmx PyDAQmxTest/test_misc.py   \n",
       "9984  drepetto/chiplotle chiplotle/examples/abstract...   \n",
       "9985       cloudera/kudu build-support/compile_flags.py   \n",
       "9986  MTG/essentia test/src/unittests/sfx/test_flatn...   \n",
       "9987  leiferikb/bitpop build/third_party/twisted_10_...   \n",
       "9988  xbmc/atv2 xbmc/lib/libPython/Python/Lib/compil...   \n",
       "9989  XvBMC/repository.xvbmc Dependencies/script.mod...   \n",
       "9990                  eve-val/evelink tests/test_map.py   \n",
       "9991     mirumee/saleor saleor/dashboard/group/views.py   \n",
       "9992  mollyproject/mollyproject molly/apps/feature_v...   \n",
       "9993  grnet/e-science tests/web/test_destroy_cluster.py   \n",
       "9994               yosinski/OpenSCAD-playground tree.py   \n",
       "9995  zhenxuan00/mmdgm conv-mmdgm/layer/NoParamsGaus...   \n",
       "9996  cpbotha/devide modules/vtk_basic/vtkStructured...   \n",
       "9997                 swoodford/raspberrypi speedtest.py   \n",
       "9998  asciidisco/plugin.video.telekom-sport resource...   \n",
       "9999  XefPatterson/INF8225_Project ChatBox/GUI/ui_ch...   \n",
       "\n",
       "                                                content  \n",
       "0     from abc import abstractmethod, ABCMeta\\n\\n\\nc...  \n",
       "1     import unittest\\nfrom typing import Any\\n\\nimp...  \n",
       "2     # pragma pylint: disable=missing-docstring, to...  \n",
       "3     # -*- coding: utf-8 -*-\\n# Generated by Django...  \n",
       "4     from api import serializers as s\\nfrom vms.mod...  \n",
       "5     # -*- coding: utf-8 -*-\\n\\n# Copyright 2012-20...  \n",
       "6     \"\"\" Unicode Mapping Parser and Codec Generator...  \n",
       "7     import struct\\n\\n\\nclass Packet39Parser(object...  \n",
       "8     from PyQt4 import QtCore\\n\\nfrom Code import D...  \n",
       "9     \"\"\"\\nDjango settings for example project.\\n\\nG...  \n",
       "10    # Copyright 2012-2016 Canonical Ltd.  This sof...  \n",
       "11    \"\"\"\\nWSGI config for cnc project.\\n\\nThis modu...  \n",
       "12    \"\"\"\\nDC version of ceas\\n\"\"\"\\nfrom samflow.com...  \n",
       "13    #!/usr/bin/env python\\n# Author: Jon Trulson <...  \n",
       "14    #!/usr/bin/python\\n\\nDOCUMENTATION = \"\"\"\\nmodu...  \n",
       "15    import numpy as np\\nimport tensorflow as tf\\nf...  \n",
       "16    #!/usr/bin/env python\\n# -*- coding: UTF-8 -*-...  \n",
       "17    # -*- coding: utf-8 -*-\\n\\nfrom armatis.models...  \n",
       "18    ##############################################...  \n",
       "19    \"\"\"\\npglookout\\n\\nCopyright (c) 2015 Ohmu Ltd\\...  \n",
       "20    # Copyright 2015 Red Hat, Inc.\\n#\\n#    Licens...  \n",
       "21    ##############################################...  \n",
       "22    # -*- coding: utf-8 -*-\\n\\nfrom __future__ imp...  \n",
       "23           def hello():\\n    return 'Hello, World!'\\n  \n",
       "24    def timedelta_total_seconds(td):\\n    \"\"\" Need...  \n",
       "25                        #\\ndef aaa():\\n    # ...\\n#\\n  \n",
       "26    class Namespace(object):\\n    \"\"\"Base class fo...  \n",
       "27    #!/usr/bin/env python3\\n# Copyright (c) 2015-2...  \n",
       "28    import walrus\\nimport uuid\\nfrom configobj imp...  \n",
       "29    \"\"\"empty message\\n\\nRevision ID: 552800b5e708\\...  \n",
       "...                                                 ...  \n",
       "9970  import random\\r\\nimport math\\r\\n\\r\\n# The dorm...  \n",
       "9971  #!/usr/bin/env python2\\n# -*- coding: utf8 -*-...  \n",
       "9972  #!/usr/bin/env python\\n# -*- coding: utf8 -*-\\...  \n",
       "9973  # Copyright (C) 2014 Yahoo! Inc. All Rights Re...  \n",
       "9974  # Copyright (c) 2012 The Khronos Group Inc.\\n#...  \n",
       "9975  from django.utils.translation import ugettext_...  \n",
       "9976  from twisted.plugin import IPlugin\\nfrom txirc...  \n",
       "9977  def get_urls(*args, **kwargs):\\n    return {'h...  \n",
       "9978  \"\"\"Map all bindings to PySide2\\n\\nThis module ...  \n",
       "9979  \"\"\"Provide unit tests for IDTxl's import funct...  \n",
       "9980  # ============================================...  \n",
       "9981  # -*- coding: utf-8 -*-\\n\"\"\"\\n/***************...  \n",
       "9982  import argparse\\nimport logging\\nlog = logging...  \n",
       "9983  import unittest\\nimport numpy as np\\nimport ct...  \n",
       "9984  #!/usr/bin/env python\\n\\nfrom chiplotle import...  \n",
       "9985  #!/usr/bin/env python\\n#\\n# Licensed to the Ap...  \n",
       "9986  #!/usr/bin/env python\\n\\n# Copyright (C) 2006-...  \n",
       "9987  # -*- test-case-name: twisted.words.test.test_...  \n",
       "9988  \"\"\"Parser for future statements\\n\\n\"\"\"\\n\\nfrom...  \n",
       "9989  import re,time\\nimport requests,xbmcaddon\\nfro...  \n",
       "9990  import mock\\n\\nfrom tests.compat import unitte...  \n",
       "9991  from django.conf import settings\\nfrom django....  \n",
       "9992  # encoding: utf-8\\nimport datetime\\nfrom south...  \n",
       "9993  # -*- coding: utf-8 -*-\\n'''\\nThis script is a...  \n",
       "9994  #! /usr/bin/python\\n\\n# Copyright 2012 by Jaso...  \n",
       "9995  import theano.tensor as T\\nimport theano.tenso...  \n",
       "9996  # class generated by DeVIDE::createDeVIDEModul...  \n",
       "9997  #!/usr/bin/env python\\n\\n# http://www.pcworld....  \n",
       "9998  # -*- coding: utf-8 -*-\\n# Module: Utils\\n# Au...  \n",
       "9999  # -*- coding: utf-8 -*-\\n# Source : Werkov, Gi...  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['repo_path','content']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(docstring_list):\n",
    "    \n",
    "    \"\"\"takes a list of doc strings and converts to a single flat list of tokens\"\"\"\n",
    "    \n",
    "    tokens = [tf.keras.preprocessing.text.text_to_word_sequence(i) for i in docstring_list]\n",
    "    flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "    flat_string = \" \".join(flat_tokens)\n",
    "    \n",
    "    return flat_string\n",
    "\n",
    "def get_docstrings(source):\n",
    "    \n",
    "    \"\"\"function to walk through parse tree and return list of docstrings\"\"\"\n",
    "    \n",
    "    NODE_TYPES = {\n",
    "    ast.ClassDef: 'Class',\n",
    "    ast.FunctionDef: 'Function/Method',\n",
    "    ast.Module: 'Module'\n",
    "    }\n",
    "    \n",
    "    docstrings = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(source)\n",
    "    except:\n",
    "        return \" \"\n",
    "       \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, tuple(NODE_TYPES)):\n",
    "            docstring = ast.get_docstring(node)\n",
    "            docstrings.append(docstring)\n",
    "    \n",
    "    docstrings =  [x for x in docstrings if x is not None]\n",
    "    clean_string = cleanup(docstrings)\n",
    "            \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docstrings'] = [get_docstrings(x) for x in list(df['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up corpus for count vectorizer\n",
    "corpus = list(df['docstrings'])\n",
    "\n",
    "#count values for tfidf calculations\n",
    "count_vect = CountVectorizer()\n",
    "count_vect = count_vect.fit(corpus)\n",
    "freq_term_matrix = count_vect.transform(corpus)\n",
    "\n",
    "#to grab columns for words\n",
    "vocab = count_vect.vocabulary_\n",
    "\n",
    "#create a holder for the new df column\n",
    "embeddings_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embed(words):\n",
    "    \n",
    "    global count_vect, freq_term_matrix, vocab\n",
    "    \n",
    "    #verify there are docstrings available\n",
    "    if len(words)==0:\n",
    "        return np.zeros(100)\n",
    "         \n",
    "    #create tfidf for each document\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tfidf.fit(freq_term_matrix)\n",
    "    doc_freq_term = count_vect.transform([words])\n",
    "    idfs = tfidf.transform(doc_freq_term)\n",
    "\n",
    "    #split the docstrings to individual words for average\n",
    "    sent_list = words.split(\" \")\n",
    "    embeddings = []\n",
    "\n",
    "    #cycle through list of words in docstring\n",
    "    for i in range(len(sent_list)):\n",
    "\n",
    "        if sent_list[i] in vocab:\n",
    "\n",
    "            col = vocab[sent_list[i]]\n",
    "            embed = hands.get_vector(sent_list[i], strict=False)\n",
    "            tfidf = idfs[0, col]\n",
    "            embeddings.append(np.multiply(embed, tfidf))\n",
    "\n",
    "        embed_array = np.asarray(embeddings)\n",
    "        \n",
    "        if len(embed_array)==0:\n",
    "            return np.zeros(100)\n",
    "\n",
    "        return np.mean(embed_array, axis=0)\n",
    "    \n",
    "def find_nn(words, embeddings):\n",
    "    \n",
    "    search = words_to_embed(words)\n",
    "    distances = [scipy.spatial.distance.cosine(search, i) for i in embeddings]\n",
    "    nn = np.argsort(np.asarray(distances))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = [words_to_embed(x) for x in list(df['docstrings'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_code(search_terms, docstrings, embeddings, n):\n",
    "    \n",
    "    top_n = find_nn(search_terms, embeddings)[0:n]\n",
    "    code = [df['content'][i] for i in top_n]\n",
    "    \n",
    "    return code\n",
    "\n",
    "doc_strings = list(df['docstrings'])\n",
    "embed_vecs = list(df['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/python\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "import dateutil.parser, datetime\n",
      "from bson.objectid import ObjectId\n",
      "from gluon.custom_import import track_changes\n",
      "from oauth import OAuth2\n",
      "from oauth.storage import web2pyStorage as storage  # change to MongoStorage if you aren't using DAL\n",
      "from oauth.exceptions import *\n",
      "track_changes(True)\n",
      "\n",
      "CODES = {'ok': 200}\n",
      "MESSAGES = {'ok': 'success'}\n",
      "\n",
      "def validate_access_token(f):\n",
      "    \"\"\"\n",
      "    Function decorator which validates an access token.\n",
      "    \"\"\"\n",
      "\n",
      "    from oauth.storage import web2pyStorage as storage  # change to MongoStorage if you aren't using DAL\n",
      "    storage = storage()\n",
      "    storage.connect()\n",
      "    oauth = OAuth2(storage)\n",
      "    \n",
      "    response.headers['Content-Type'] = json_headers()\n",
      "    response.view = json_service()\n",
      "\n",
      "    header = request.env['http_authorization']\n",
      "    token = oauth.validate_access_params(request.get_vars, request.post_vars,\n",
      "                                         header)\n",
      "                                    \n",
      "    return f  # what does f have?\n",
      "              \n",
      "def parse_to_date(default, arg):\n",
      "    try:\n",
      "        if default:\n",
      "            return dateutil.parser.parse(default) if not arg else dateutil.parser.parse(arg)\n",
      "        else:\n",
      "            return None if not arg else dateutil.parser.parse(arg)\n",
      "    except ValueError:\n",
      "        return default\n",
      " \n",
      "def json_service():\n",
      "    return 'generic.json'\n",
      "        \n",
      "def json_headers():\n",
      "    return 'application/json; charset=utf-8'\n",
      "\n",
      "def encode_model(obj, recursive=False):\n",
      "    if obj is None:\n",
      "        return obj\n",
      "\n",
      "    import bson\n",
      "    import datetime\n",
      "\n",
      "    if isinstance(obj, (int, float, basestring)):\n",
      "        out = obj\n",
      "    elif isinstance(obj, list):\n",
      "        out = [encode_model(item) for item in obj]\n",
      "    elif isinstance(obj, dict):\n",
      "        out = dict([(k, encode_model(v)) for (k, v) in obj.items()])\n",
      "    elif isinstance(obj, (datetime.datetime, datetime.timedelta)):\n",
      "        out = str(obj)\n",
      "    elif isinstance(obj, bson.objectid.ObjectId):\n",
      "        out = {'ObjectId': str(obj)}\n",
      "    else:\n",
      "        raise NameError(\"Could not JSON-encode type '%s': %s\" % (type(obj), str(obj)))\n",
      "\n",
      "    return out\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search1 = \"function that calculates distance\"\n",
    "query1 = top_n_code(search1, doc_strings, embed_vecs, 10)\n",
    "print(query1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Copyright (c) 2017, Henrique Miranda\n",
      "# All rights reserved.\n",
      "#\n",
      "# This file is part of the phononwebsite project\n",
      "#\n",
      "\"\"\" Code the dictionary in json format \"\"\"\n",
      "import json\n",
      "import numpy as np\n",
      "\n",
      "class JsonEncoder(json.JSONEncoder):\n",
      "    def default(self, obj):\n",
      "        if isinstance(obj, (np.ndarray,np.number)):\n",
      "            if np.iscomplexobj(obj):\n",
      "                return [obj.real, obj.imag]\n",
      "            else:\n",
      "                return obj.tolist()\n",
      "        return(json.JSONEncoder.default(self, obj))\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search2 = \"code to merge two files\"\n",
    "query2 = top_n_code(search2, doc_strings, embed_vecs, 10)\n",
    "print(query2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import tensorflow as tf\n",
      "import matplotlib.pyplot as plt\n",
      "import cPickle as pickle\n",
      "import copy\n",
      "import json\n",
      "from tqdm import tqdm\n",
      "\n",
      "from utils.nn import NN\n",
      "from utils.coco.coco import COCO\n",
      "from utils.coco.pycocoevalcap.eval import COCOEvalCap\n",
      "from utils.misc import ImageLoader, CaptionData, TopN\n",
      "\n",
      "class BaseModel(object):\n",
      "    def __init__(self, config):\n",
      "        self.config = config\n",
      "        self.is_train = True if config.phase == 'train' else False\n",
      "        self.train_cnn = self.is_train and config.train_cnn\n",
      "        self.image_loader = ImageLoader('./utils/ilsvrc_2012_mean.npy')\n",
      "        self.image_shape = [224, 224, 3]\n",
      "        self.nn = NN(config)\n",
      "        self.global_step = tf.Variable(0,\n",
      "                                       name = 'global_step',\n",
      "                                       trainable = False)\n",
      "        self.build()\n",
      "\n",
      "    def build(self):\n",
      "        raise NotImplementedError()\n",
      "\n",
      "    def train(self, sess, train_data):\n",
      "        \"\"\" Train the model using the COCO train2014 data. \"\"\"\n",
      "        print(\"Training the model...\")\n",
      "        config = self.config\n",
      "\n",
      "        if not os.path.exists(config.summary_dir):\n",
      "            os.mkdir(config.summary_dir)\n",
      "        train_writer = tf.summary.FileWriter(config.summary_dir,\n",
      "                                             sess.graph)\n",
      "\n",
      "        for _ in tqdm(list(range(config.num_epochs)), desc='epoch'):\n",
      "            for _ in tqdm(list(range(train_data.num_batches)), desc='batch'):\n",
      "                batch = train_data.next_batch()\n",
      "                image_files, sentences, masks = batch\n",
      "                images = self.image_loader.load_images(image_files)\n",
      "                feed_dict = {self.images: images,\n",
      "                             self.sentences: sentences,\n",
      "                             self.masks: masks}\n",
      "                _, summary, global_step = sess.run([self.opt_op,\n",
      "                                                    self.summary,\n",
      "                                                    self.global_step],\n",
      "                                                    feed_dict=feed_dict)\n",
      "                if (global_step + 1) % config.save_period == 0:\n",
      "                    self.save()\n",
      "                train_writer.add_summary(summary, global_step)\n",
      "            train_data.reset()\n",
      "\n",
      "        self.save()\n",
      "        train_writer.close()\n",
      "        print(\"Training complete.\")\n",
      "\n",
      "    def eval(self, sess, eval_gt_coco, eval_data, vocabulary):\n",
      "        \"\"\" Evaluate the model using the COCO val2014 data. \"\"\"\n",
      "        print(\"Evaluating the model ...\")\n",
      "        config = self.config\n",
      "\n",
      "        results = []\n",
      "        if not os.path.exists(config.eval_result_dir):\n",
      "            os.mkdir(config.eval_result_dir)\n",
      "\n",
      "        # Generate the captions for the images\n",
      "        idx = 0\n",
      "        for k in tqdm(list(range(eval_data.num_batches)), desc='batch'):\n",
      "            batch = eval_data.next_batch()\n",
      "            caption_data = self.beam_search(sess, batch, vocabulary)\n",
      "\n",
      "            fake_cnt = 0 if k<eval_data.num_batches-1 \\\n",
      "                         else eval_data.fake_count\n",
      "            for l in range(eval_data.batch_size-fake_cnt):\n",
      "                word_idxs = caption_data[l][0].sentence\n",
      "                score = caption_data[l][0].score\n",
      "                caption = vocabulary.get_sentence(word_idxs)\n",
      "                results.append({'image_id': eval_data.image_ids[idx],\n",
      "                                'caption': caption})\n",
      "                idx += 1\n",
      "\n",
      "                # Save the result in an image file, if requested\n",
      "                if config.save_eval_result_as_image:\n",
      "                    image_file = batch[l]\n",
      "                    image_name = image_file.split(os.sep)[-1]\n",
      "                    image_name = os.path.splitext(image_name)[0]\n",
      "                    img = plt.imread(image_file)\n",
      "                    plt.imshow(img)\n",
      "                    plt.axis('off')\n",
      "                    plt.title(caption)\n",
      "                    plt.savefig(os.path.join(config.eval_result_dir,\n",
      "                                             image_name+'_result.jpg'))\n",
      "\n",
      "        fp = open(config.eval_result_file, 'wb')\n",
      "        json.dump(results, fp)\n",
      "        fp.close()\n",
      "\n",
      "        # Evaluate these captions\n",
      "        eval_result_coco = eval_gt_coco.loadRes(config.eval_result_file)\n",
      "        scorer = COCOEvalCap(eval_gt_coco, eval_result_coco)\n",
      "        scorer.evaluate()\n",
      "        print(\"Evaluation complete.\")\n",
      "\n",
      "    def test(self, sess, test_data, vocabulary):\n",
      "        \"\"\" Test the model using any given images. \"\"\"\n",
      "        print(\"Testing the model ...\")\n",
      "        config = self.config\n",
      "\n",
      "        if not os.path.exists(config.test_result_dir):\n",
      "            os.mkdir(config.test_result_dir)\n",
      "\n",
      "        captions = []\n",
      "        scores = []\n",
      "\n",
      "        # Generate the captions for the images\n",
      "        for k in tqdm(list(range(test_data.num_batches)), desc='path'):\n",
      "            batch = test_data.next_batch()\n",
      "            caption_data = self.beam_search(sess, batch, vocabulary)\n",
      "\n",
      "            fake_cnt = 0 if k<test_data.num_batches-1 \\\n",
      "                         else test_data.fake_count\n",
      "            for l in range(test_data.batch_size-fake_cnt):\n",
      "                word_idxs = caption_data[l][0].sentence\n",
      "                score = caption_data[l][0].score\n",
      "                caption = vocabulary.get_sentence(word_idxs)\n",
      "                captions.append(caption)\n",
      "                scores.append(score)\n",
      "\n",
      "                # Save the result in an image file\n",
      "                image_file = batch[l]\n",
      "                image_name = image_file.split(os.sep)[-1]\n",
      "                image_name = os.path.splitext(image_name)[0]\n",
      "                img = plt.imread(image_file)\n",
      "                plt.imshow(img)\n",
      "                plt.axis('off')\n",
      "                plt.title(caption)\n",
      "                plt.savefig(os.path.join(config.test_result_dir,\n",
      "                                         image_name+'_result.jpg'))\n",
      "\n",
      "        # Save the captions to a file\n",
      "        results = pd.DataFrame({'image_files':test_data.image_files,\n",
      "                                'caption':captions,\n",
      "                                'prob':scores})\n",
      "        results.to_csv(config.test_result_file)\n",
      "        print(\"Testing complete.\")\n",
      "\n",
      "    def beam_search(self, sess, image_files, vocabulary):\n",
      "        \"\"\"Use beam search to generate the captions for a batch of images.\"\"\"\n",
      "        # Feed in the images to get the contexts and the initial LSTM states\n",
      "        config = self.config\n",
      "        images = self.image_loader.load_images(image_files)\n",
      "        contexts, initial_memory, initial_output = sess.run(\n",
      "            [self.conv_feats, self.initial_memory, self.initial_output],\n",
      "            feed_dict = {self.images: images})\n",
      "\n",
      "        partial_caption_data = []\n",
      "        complete_caption_data = []\n",
      "        for k in range(config.batch_size):\n",
      "            initial_beam = CaptionData(sentence = [],\n",
      "                                       memory = initial_memory[k],\n",
      "                                       output = initial_output[k],\n",
      "                                       score = 1.0)\n",
      "            partial_caption_data.append(TopN(config.beam_size))\n",
      "            partial_caption_data[-1].push(initial_beam)\n",
      "            complete_caption_data.append(TopN(config.beam_size))\n",
      "\n",
      "        # Run beam search\n",
      "        for idx in range(config.max_caption_length):\n",
      "            partial_caption_data_lists = []\n",
      "            for k in range(config.batch_size):\n",
      "                data = partial_caption_data[k].extract()\n",
      "                partial_caption_data_lists.append(data)\n",
      "                partial_caption_data[k].reset()\n",
      "\n",
      "            num_steps = 1 if idx == 0 else config.beam_size\n",
      "            for b in range(num_steps):\n",
      "                if idx == 0:\n",
      "                    last_word = np.zeros((config.batch_size), np.int32)\n",
      "                else:\n",
      "                    last_word = np.array([pcl[b].sentence[-1]\n",
      "                                        for pcl in partial_caption_data_lists],\n",
      "                                        np.int32)\n",
      "\n",
      "                last_memory = np.array([pcl[b].memory\n",
      "                                        for pcl in partial_caption_data_lists],\n",
      "                                        np.float32)\n",
      "                last_output = np.array([pcl[b].output\n",
      "                                        for pcl in partial_caption_data_lists],\n",
      "                                        np.float32)\n",
      "\n",
      "                memory, output, scores = sess.run(\n",
      "                    [self.memory, self.output, self.probs],\n",
      "                    feed_dict = {self.contexts: contexts,\n",
      "                                 self.last_word: last_word,\n",
      "                                 self.last_memory: last_memory,\n",
      "                                 self.last_output: last_output})\n",
      "\n",
      "                # Find the beam_size most probable next words\n",
      "                for k in range(config.batch_size):\n",
      "                    caption_data = partial_caption_data_lists[k][b]\n",
      "                    words_and_scores = list(enumerate(scores[k]))\n",
      "                    words_and_scores.sort(key=lambda x: -x[1])\n",
      "                    words_and_scores = words_and_scores[0:config.beam_size+1]\n",
      "\n",
      "                    # Append each of these words to the current partial caption\n",
      "                    for w, s in words_and_scores:\n",
      "                        sentence = caption_data.sentence + [w]\n",
      "                        score = caption_data.score * s\n",
      "                        beam = CaptionData(sentence,\n",
      "                                           memory[k],\n",
      "                                           output[k],\n",
      "                                           score)\n",
      "                        if vocabulary.words[w] == '.':\n",
      "                            complete_caption_data[k].push(beam)\n",
      "                        else:\n",
      "                            partial_caption_data[k].push(beam)\n",
      "\n",
      "        results = []\n",
      "        for k in range(config.batch_size):\n",
      "            if complete_caption_data[k].size() == 0:\n",
      "                complete_caption_data[k] = partial_caption_data[k]\n",
      "            results.append(complete_caption_data[k].extract(sort=True))\n",
      "\n",
      "        return results\n",
      "\n",
      "    def save(self):\n",
      "        \"\"\" Save the model. \"\"\"\n",
      "        config = self.config\n",
      "        data = {v.name: v.eval() for v in tf.global_variables()}\n",
      "        save_path = os.path.join(config.save_dir, str(self.global_step.eval()))\n",
      "\n",
      "        print((\" Saving the model to %s...\" % (save_path+\".npy\")))\n",
      "        np.save(save_path, data)\n",
      "        info_file = open(os.path.join(config.save_dir, \"config.pickle\"), \"wb\")\n",
      "        config_ = copy.copy(config)\n",
      "        config_.global_step = self.global_step.eval()\n",
      "        pickle.dump(config_, info_file)\n",
      "        info_file.close()\n",
      "        print(\"Model saved.\")\n",
      "\n",
      "    def load(self, sess, model_file=None):\n",
      "        \"\"\" Load the model. \"\"\"\n",
      "        config = self.config\n",
      "        if model_file is not None:\n",
      "            save_path = model_file\n",
      "        else:\n",
      "            info_path = os.path.join(config.save_dir, \"config.pickle\")\n",
      "            info_file = open(info_path, \"rb\")\n",
      "            config = pickle.load(info_file)\n",
      "            global_step = config.global_step\n",
      "            info_file.close()\n",
      "            save_path = os.path.join(config.save_dir,\n",
      "                                     str(global_step)+\".npy\")\n",
      "\n",
      "        print(\"Loading the model from %s...\" %save_path)\n",
      "        data_dict = np.load(save_path).item()\n",
      "        count = 0\n",
      "        for v in tqdm(tf.global_variables()):\n",
      "            if v.name in data_dict.keys():\n",
      "                sess.run(v.assign(data_dict[v.name]))\n",
      "                count += 1\n",
      "        print(\"%d tensors loaded.\" %count)\n",
      "\n",
      "    def load_cnn(self, session, data_path, ignore_missing=True):\n",
      "        \"\"\" Load a pretrained CNN model. \"\"\"\n",
      "        print(\"Loading the CNN from %s...\" %data_path)\n",
      "        data_dict = np.load(data_path).item()\n",
      "        count = 0\n",
      "        for op_name in tqdm(data_dict):\n",
      "            with tf.variable_scope(op_name, reuse = True):\n",
      "                for param_name, data in data_dict[op_name].iteritems():\n",
      "                    try:\n",
      "                        var = tf.get_variable(param_name)\n",
      "                        session.run(var.assign(data))\n",
      "                        count += 1\n",
      "                    except ValueError:\n",
      "                        pass\n",
      "        print(\"%d tensors loaded.\" %count)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search3 = \"train a neural network for image reconition\"\n",
    "query3 = top_n_code(search3, doc_strings, embed_vecs, 10)\n",
    "print(query3[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rekall Memory Forensics\n",
      "# Copyright (C) 2007-2011 Volatile Systems\n",
      "# Copyright 2013 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Additional Authors:\n",
      "# Michael Cohen <scudette@users.sourceforge.net>\n",
      "# Mike Auty <mike.auty@gmail.com>\n",
      "#\n",
      "# This program is free software; you can redistribute it and/or modify\n",
      "# it under the terms of the GNU General Public License as published by\n",
      "# the Free Software Foundation; either version 2 of the License, or (at\n",
      "# your option) any later version.\n",
      "#\n",
      "# This program is distributed in the hope that it will be useful, but\n",
      "# WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n",
      "# General Public License for more details.\n",
      "#\n",
      "# You should have received a copy of the GNU General Public License\n",
      "# along with this program; if not, write to the Free Software\n",
      "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n",
      "#\n",
      "\n",
      "# pylint: disable=protected-access\n",
      "\n",
      "from future import standard_library\n",
      "standard_library.install_aliases()\n",
      "from rekall import testlib\n",
      "from rekall_lib import utils\n",
      "\n",
      "from rekall.plugins.common import memmap\n",
      "from rekall.plugins.windows import common\n",
      "\n",
      "\n",
      "class WinPsList(common.WinProcessFilter):\n",
      "    \"\"\"List processes for windows.\"\"\"\n",
      "\n",
      "    __name = \"pslist\"\n",
      "\n",
      "    eprocess = None\n",
      "\n",
      "    table_header = [\n",
      "        dict(type=\"_EPROCESS\", name=\"_EPROCESS\"),\n",
      "        dict(name=\"ppid\", width=6, align=\"r\"),\n",
      "        dict(name=\"thread_count\", width=6, align=\"r\"),\n",
      "        dict(name=\"handle_count\", width=8, align=\"r\"),\n",
      "        dict(name=\"session_id\", width=6, align=\"r\"),\n",
      "        dict(name=\"wow64\", width=6),\n",
      "        dict(name=\"process_create_time\", width=24),\n",
      "        dict(name=\"process_exit_time\", width=24)\n",
      "    ]\n",
      "\n",
      "    def column_types(self):\n",
      "        result = self._row(self.session.profile._EPROCESS())\n",
      "        result[\"handle_count\"] = result[\"ppid\"]\n",
      "        result[\"session_id\"] = result[\"ppid\"]\n",
      "\n",
      "        return result\n",
      "\n",
      "    def _row(self, task):\n",
      "        return dict(_EPROCESS=task,\n",
      "                    ppid=task.InheritedFromUniqueProcessId,\n",
      "                    thread_count=task.ActiveThreads,\n",
      "                    handle_count=task.ObjectTable.m(\"HandleCount\"),\n",
      "                    session_id=task.SessionId,\n",
      "                    wow64=task.IsWow64,\n",
      "                    process_create_time=task.CreateTime,\n",
      "                    process_exit_time=task.ExitTime)\n",
      "\n",
      "    def collect(self):\n",
      "        for task in self.filter_processes():\n",
      "            yield self._row(task)\n",
      "\n",
      "\n",
      "class WinDllList(common.WinProcessFilter):\n",
      "    \"\"\"Prints a list of dll modules mapped into each process.\"\"\"\n",
      "\n",
      "    __name = \"dlllist\"\n",
      "\n",
      "    table_header = [\n",
      "        dict(name=\"divider\", type=\"Divider\"),\n",
      "        dict(name=\"_EPROCESS\", hidden=True),\n",
      "        dict(name=\"base\", style=\"address\"),\n",
      "        dict(name=\"size\", style=\"address\"),\n",
      "        dict(name=\"reason\", width=30),\n",
      "        dict(name=\"dll_path\"),\n",
      "    ]\n",
      "\n",
      "    def collect(self):\n",
      "        for task in self.filter_processes():\n",
      "            pid = task.UniqueProcessId\n",
      "\n",
      "            divider = \"{0} pid: {1:6}\\n\".format(task.ImageFileName, pid)\n",
      "\n",
      "            if task.Peb:\n",
      "                divider += u\"Command line : {0}\\n\".format(\n",
      "                    task.Peb.ProcessParameters.CommandLine)\n",
      "\n",
      "                divider += u\"{0}\\n\\n\".format(task.Peb.CSDVersion)\n",
      "                yield dict(divider=divider)\n",
      "\n",
      "                for m in task.get_load_modules():\n",
      "                    yield dict(base=m.DllBase,\n",
      "                               size=m.SizeOfImage,\n",
      "                               reason=m.LoadReason,\n",
      "                               dll_path=m.FullDllName,\n",
      "                               _EPROCESS=task)\n",
      "            else:\n",
      "                yield dict(divider=\"Unable to read PEB for task.\\n\")\n",
      "\n",
      "\n",
      "class WinMemMap(memmap.MemmapMixIn, common.WinProcessFilter):\n",
      "    \"\"\"Calculates the memory regions mapped by a process.\"\"\"\n",
      "    __name = \"memmap\"\n",
      "\n",
      "    def _get_highest_user_address(self):\n",
      "        return self.profile.get_constant_object(\n",
      "            \"MmHighestUserAddress\", \"Pointer\").v()\n",
      "\n",
      "\n",
      "class Threads(common.WinProcessFilter):\n",
      "    \"\"\"Enumerate threads.\"\"\"\n",
      "    name = \"threads\"\n",
      "\n",
      "    table_header = [\n",
      "        dict(name=\"_ETHREAD\", style=\"address\"),\n",
      "        dict(name=\"pid\", align=\"r\", width=6),\n",
      "        dict(name=\"tid\", align=\"r\", width=6),\n",
      "        dict(name=\"start\", style=\"address\"),\n",
      "        dict(name=\"start_symbol\", width=30),\n",
      "        dict(name=\"Process\", width=16),\n",
      "        dict(name=\"win32_start\", style=\"address\"),\n",
      "        dict(name=\"win32_start_symb\")\n",
      "    ]\n",
      "\n",
      "    def collect(self):\n",
      "        cc = self.session.plugins.cc()\n",
      "        with cc:\n",
      "            for task in self.filter_processes():\n",
      "                # Resolve names in the process context.\n",
      "                cc.SwitchProcessContext(process=task)\n",
      "\n",
      "                for thread in task.ThreadListHead.list_of_type(\n",
      "                        \"_ETHREAD\", \"ThreadListEntry\"):\n",
      "\n",
      "                    yield dict(_ETHREAD=thread,\n",
      "                               pid=thread.Cid.UniqueProcess,\n",
      "                               tid=thread.Cid.UniqueThread,\n",
      "                               start=thread.StartAddress,\n",
      "                               start_symbol=utils.FormattedAddress(\n",
      "                                   self.session.address_resolver,\n",
      "                                   thread.StartAddress),\n",
      "                               Process=task.ImageFileName,\n",
      "                               win32_start=thread.Win32StartAddress,\n",
      "                               win32_start_symb=utils.FormattedAddress(\n",
      "                                   self.session.address_resolver,\n",
      "                                   thread.Win32StartAddress,\n",
      "                                   ))\n",
      "\n",
      "\n",
      "class WinMemDump(memmap.MemDumpMixin, common.WinProcessFilter):\n",
      "    \"\"\"Dump windows processes.\"\"\"\n",
      "\n",
      "\n",
      "class TestWinMemDump(testlib.HashChecker):\n",
      "    \"\"\"Test the pslist module.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(\n",
      "        commandline=\"memdump %(pids)s --dump_dir %(tempdir)s\",\n",
      "        pid=2624)\n",
      "\n",
      "\n",
      "class TestMemmap(testlib.SimpleTestCase):\n",
      "    \"\"\"Test the pslist module.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(\n",
      "        commandline=\"memmap %(pids)s\",\n",
      "        pid=2624)\n",
      "\n",
      "\n",
      "class TestMemmapCoalesce(testlib.SimpleTestCase):\n",
      "    \"\"\"Make sure that memmaps are coalesced properly.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(commandline=\"memmap %(pids)s --coalesce\",\n",
      "                      pid=2624)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search4 = \"list the first 100 Fibonacci Numbers\"\n",
    "query4 = top_n_code(search4, doc_strings, embed_vecs, 10)\n",
    "print(query4[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#     Copyright 2018, Kay Hayen, mailto:kay.hayen@gmail.com\n",
      "#\n",
      "#     Part of \"Nuitka\", an optimizing Python compiler that is compatible and\n",
      "#     integrates with CPython, but also works on its own.\n",
      "#\n",
      "#     Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#     you may not use this file except in compliance with the License.\n",
      "#     You may obtain a copy of the License at\n",
      "#\n",
      "#        http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "#     Unless required by applicable law or agreed to in writing, software\n",
      "#     distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "#     See the License for the specific language governing permissions and\n",
      "#     limitations under the License.\n",
      "#\n",
      "\"\"\" Syntax highlighting for Python.\n",
      "\n",
      "Inspired/copied from by http://diotavelli.net/PyQtWiki/Python%20syntax%20highlighting\n",
      "\"\"\"\n",
      "\n",
      "from PyQt5.QtCore import (\n",
      "    QRegExp  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QColor  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QFont  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QSyntaxHighlighter  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QTextCharFormat  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "\n",
      "\n",
      "def createTextFormat(color, style = \"\"):\n",
      "    \"\"\"Return a QTextCharFormat with the given attributes.\n",
      "    \"\"\"\n",
      "    _color = QColor()\n",
      "    _color.setNamedColor(color)\n",
      "\n",
      "    _format = QTextCharFormat()\n",
      "    _format.setForeground(_color)\n",
      "    if \"bold\" in style:\n",
      "        _format.setFontWeight(QFont.Bold)\n",
      "    if \"italic\" in style:\n",
      "        _format.setFontItalic(True)\n",
      "\n",
      "    return _format\n",
      "\n",
      "\n",
      "# Syntax styles that can be shared by all languages\n",
      "STYLES = {\n",
      "    \"keyword\"  : createTextFormat(\"blue\"),\n",
      "    \"operator\" : createTextFormat(\"red\"),\n",
      "    \"brace\"    : createTextFormat(\"darkGray\"),\n",
      "    \"defclass\" : createTextFormat(\"black\", \"bold\"),\n",
      "    \"string\"   : createTextFormat(\"magenta\"),\n",
      "    \"string2\"  : createTextFormat(\"darkMagenta\"),\n",
      "    \"comment\"  : createTextFormat(\"darkGreen\", \"italic\"),\n",
      "    \"self\"     : createTextFormat(\"black\", \"italic\"),\n",
      "    \"numbers\"  : createTextFormat(\"brown\"),\n",
      "}\n",
      "\n",
      "\n",
      "class PythonHighlighter(QSyntaxHighlighter):\n",
      "    \"\"\" Syntax highlighter for the Python language.\n",
      "    \"\"\"\n",
      "    # Python keywords\n",
      "    keywords = [\n",
      "        \"and\", \"assert\", \"break\", \"class\", \"continue\", \"def\",\n",
      "        \"del\", \"elif\", \"else\", \"except\", \"exec\", \"finally\",\n",
      "        \"for\", \"from\", \"global\", \"if\", \"import\", \"in\",\n",
      "        \"is\", \"lambda\", \"not\", \"or\", \"pass\", \"print\",\n",
      "        \"raise\", \"return\", \"try\", \"while\", \"with\", \"yield\",\n",
      "        \"None\", \"True\", \"False\",\n",
      "    ]\n",
      "\n",
      "    # Python operators\n",
      "    operators = [\n",
      "        '=',\n",
      "        # Comparison\n",
      "        \"==\", \"!=\", '<', \"<=\", '>', \">=\",\n",
      "        # Arithmetic\n",
      "        \"\\+\", '-', \"\\*\", '/', \"//\", \"\\%\", \"\\*\\*\",\n",
      "        # In-place\n",
      "        \"\\+=\", \"-=\", \"\\*=\", \"/=\", \"\\%=\",\n",
      "        # Bitwise\n",
      "        \"\\^\", \"\\|\", \"\\&\", \"\\~\", \">>\", \"<<\",\n",
      "    ]\n",
      "\n",
      "    # Python braces\n",
      "    braces = [\n",
      "        \"\\{\", \"\\}\", \"\\(\", \"\\)\", \"\\[\", \"\\]\",\n",
      "    ]\n",
      "    def __init__(self, document):\n",
      "        QSyntaxHighlighter.__init__(self, document)\n",
      "\n",
      "        # Multi-line strings (expression, flag, style)\n",
      "        # The triple-quotes in these two lines will mess up the\n",
      "        # syntax highlighting from this point onward\n",
      "        self.tri_single = (QRegExp(\"'''\"), 1, STYLES[\"string2\"])\n",
      "        self.tri_double = (QRegExp('\"\"\"'), 2, STYLES[\"string2\"])\n",
      "\n",
      "        rules = []\n",
      "\n",
      "        # Keyword, operator, and brace rules\n",
      "        rules += [(r'\\b%s\\b' % w, 0, STYLES[\"keyword\"])\n",
      "            for w in PythonHighlighter.keywords]\n",
      "        rules += [(r'%s' % o, 0, STYLES[\"operator\"])\n",
      "            for o in PythonHighlighter.operators]\n",
      "        rules += [(r'%s' % b, 0, STYLES[\"brace\"])\n",
      "            for b in PythonHighlighter.braces]\n",
      "\n",
      "        # All other rules\n",
      "        rules += [\n",
      "            # 'self'\n",
      "            (r'\\bself\\b', 0, STYLES[\"self\"]),\n",
      "\n",
      "            # Double-quoted string, possibly containing escape sequences\n",
      "            (r'\"[^\"\\\\]*(\\\\.[^\"\\\\]*)*\"', 0, STYLES[\"string\"]),\n",
      "            # Single-quoted string, possibly containing escape sequences\n",
      "            (r\"'[^'\\\\]*(\\\\.[^'\\\\]*)*'\", 0, STYLES[\"string\"]),\n",
      "\n",
      "            # 'def' followed by an identifier\n",
      "            (r'\\bdef\\b\\s*(\\w+)', 1, STYLES[\"defclass\"]),\n",
      "            # 'class' followed by an identifier\n",
      "            (r'\\bclass\\b\\s*(\\w+)', 1, STYLES[\"defclass\"]),\n",
      "\n",
      "            # From '#' until a newline\n",
      "            (r'#[^\\n]*', 0, STYLES[\"comment\"]),\n",
      "\n",
      "            # Numeric literals\n",
      "            (r'\\b[+-]?[0-9]+[lL]?\\b', 0, STYLES[\"numbers\"]),\n",
      "            (r'\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b', 0, STYLES[\"numbers\"]),\n",
      "            (r'\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b', 0, STYLES[\"numbers\"]),\n",
      "        ]\n",
      "\n",
      "        # Build a QRegExp for each pattern\n",
      "        self.rules = [(QRegExp(pat), index, fmt)\n",
      "            for (pat, index, fmt) in rules]\n",
      "\n",
      "\n",
      "    def highlightBlock(self, text):\n",
      "        \"\"\"Apply syntax highlighting to the given block of text.\n",
      "        \"\"\"\n",
      "        # Do other syntax formatting\n",
      "        for expression, nth, display_format in self.rules:\n",
      "            index = expression.indexIn(text, 0)\n",
      "\n",
      "            while index >= 0:\n",
      "                # We actually want the index of the nth match\n",
      "                index = expression.pos(nth)\n",
      "                length = expression.cap(nth).length()\n",
      "                self.setFormat(index, length, display_format)\n",
      "                index = expression.indexIn(text, index + length)\n",
      "\n",
      "        self.setCurrentBlockState(0)\n",
      "\n",
      "        # Do multi-line strings\n",
      "        in_multiline = self.match_multiline(text, *self.tri_single)\n",
      "        if not in_multiline:\n",
      "            in_multiline = self.match_multiline(text, *self.tri_double)\n",
      "\n",
      "\n",
      "    def match_multiline(self, text, delimiter, in_state, style):\n",
      "        \"\"\"Do highlighting of multi-line strings. ``delimiter`` should be a\n",
      "        ``QRegExp`` for triple-single-quotes or triple-double-quotes, and\n",
      "        ``in_state`` should be a unique integer to represent the corresponding\n",
      "        state changes when inside those strings. Returns True if we're still\n",
      "        inside a multi-line string when this function is finished.\n",
      "        \"\"\"\n",
      "        # If inside triple-single quotes, start at 0\n",
      "        if self.previousBlockState() == in_state:\n",
      "            start = 0\n",
      "            add = 0\n",
      "        # Otherwise, look for the delimiter on this line\n",
      "        else:\n",
      "            start = delimiter.indexIn(text)\n",
      "            # Move past this match\n",
      "            add = delimiter.matchedLength()\n",
      "\n",
      "        # As long as there's a delimiter match on this line...\n",
      "        while start >= 0:\n",
      "            # Look for the ending delimiter\n",
      "            end = delimiter.indexIn(text, start + add)\n",
      "            # Ending delimiter on this line?\n",
      "            if end >= add:\n",
      "                length = end - start + add + delimiter.matchedLength()\n",
      "                self.setCurrentBlockState(0)\n",
      "            # No; multi-line string\n",
      "            else:\n",
      "                self.setCurrentBlockState(in_state)\n",
      "                length = text.length() - start + add\n",
      "            # Apply formatting\n",
      "            self.setFormat(start, length, style)\n",
      "            # Look for the next match\n",
      "            start = delimiter.indexIn(text, start + length)\n",
      "\n",
      "        # Return True if still inside a multi-line string, False otherwise\n",
      "        if self.currentBlockState() == in_state:\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "def addPythonHighlighter(document):\n",
      "    PythonHighlighter(document)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search5 = \"semantic search tool for text\"\n",
    "query5 = top_n_code(search5, doc_strings, embed_vecs, 10)\n",
    "print(query5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
