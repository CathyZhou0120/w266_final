{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import ast\n",
    "import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "import keras\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import regex as re\n",
    "import ast\n",
    "import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cathyzhou/anaconda3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project='manifest-frame-203601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"\"\"\n",
    "    select distinct repo_path,c_content from w266_final.final_20k\n",
    "    \"\"\")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "df = []\n",
    "for row in rows:\n",
    "    df.append([row.repo_path,row.c_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172413, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['repo_path','content']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(docstring_list):\n",
    "    \n",
    "    \"\"\"takes a list of doc strings and converts to a single flat list of tokens\"\"\"\n",
    "    \n",
    "    tokens = [tf.keras.preprocessing.text.text_to_word_sequence(i) for i in docstring_list]\n",
    "    flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "    flat_string = \" \".join(flat_tokens)\n",
    "    \n",
    "    return flat_string\n",
    "\n",
    "def get_docstrings(source):\n",
    "    \n",
    "    \"\"\"function to walk through parse tree and return list of docstrings\"\"\"\n",
    "    \n",
    "    NODE_TYPES = {\n",
    "    ast.ClassDef: 'Class',\n",
    "    ast.FunctionDef: 'Function/Method',\n",
    "    ast.Module: 'Module'\n",
    "    }\n",
    "    \n",
    "    docstrings = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(source)\n",
    "    except:\n",
    "        return \" \"\n",
    "       \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, tuple(NODE_TYPES)):\n",
    "            docstring = ast.get_docstring(node)\n",
    "            docstrings.append(docstring)\n",
    "    \n",
    "    docstrings =  [x for x in docstrings if x is not None]\n",
    "    clean_string = cleanup(docstrings)\n",
    "            \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "      <th>docstrings</th>\n",
       "      <th>docstring2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ilogue/niprov tests/test_mediumviewer.py</td>\n",
       "      <td>#!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\nfr...</td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UndeadMastodon/Loltris Matrix.py</td>\n",
       "      <td>#!/usr/bin/python2\\n#-*- coding: utf-8 -*-\\n\\n...</td>\n",
       "      <td>prints a matrix to the console for debugging p...</td>\n",
       "      <td>[[prints, a, matrix, to, the, console, for, de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anhstudios/swganh data/scripts/templates/objec...</td>\n",
       "      <td>#### NOTICE: THIS FILE IS AUTOGENERATED\\n#### ...</td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Southpaw-TACTIC/TACTIC src/bin/example/add_fla...</td>\n",
       "      <td>import sys\\r\\nimport tacticenv\\r\\n\\r\\n\\r\\nfrom...</td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>scrapinghub/exporters exporters/writers/dropbo...</td>\n",
       "      <td>from collections import Counter\\nfrom exporter...</td>\n",
       "      <td>writes items to dropbox folder options availab...</td>\n",
       "      <td>[[writes, items, to, dropbox, folder, options,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_path  \\\n",
       "0           ilogue/niprov tests/test_mediumviewer.py   \n",
       "1                   UndeadMastodon/Loltris Matrix.py   \n",
       "2  anhstudios/swganh data/scripts/templates/objec...   \n",
       "3  Southpaw-TACTIC/TACTIC src/bin/example/add_fla...   \n",
       "4  scrapinghub/exporters exporters/writers/dropbo...   \n",
       "\n",
       "                                             content  \\\n",
       "0  #!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\nfr...   \n",
       "1  #!/usr/bin/python2\\n#-*- coding: utf-8 -*-\\n\\n...   \n",
       "2  #### NOTICE: THIS FILE IS AUTOGENERATED\\n#### ...   \n",
       "3  import sys\\r\\nimport tacticenv\\r\\n\\r\\n\\r\\nfrom...   \n",
       "4  from collections import Counter\\nfrom exporter...   \n",
       "\n",
       "                                          docstrings  \\\n",
       "0                                                      \n",
       "1  prints a matrix to the console for debugging p...   \n",
       "2                                                      \n",
       "3                                                      \n",
       "4  writes items to dropbox folder options availab...   \n",
       "\n",
       "                                          docstring2  \n",
       "0                                               [[]]  \n",
       "1  [[prints, a, matrix, to, the, console, for, de...  \n",
       "2                                               [[]]  \n",
       "3                                               [[]]  \n",
       "4  [[writes, items, to, dropbox, folder, options,...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['docstrings'] = [get_docstrings(x) for x in list(df['content'])]\n",
    "doc = []\n",
    "for i in df['docstrings']:\n",
    "    doc.append([i.split()])\n",
    "df['docstring2'] = doc\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(model):\n",
    "    # convert the wv word vectors into a numpy matrix that is suitable for insertion\n",
    "    # into our TensorFlow and Keras models\n",
    "    embedding_matrix = np.zeros((len(model.wv.vocab), 100))\n",
    "    for i in range(len(model.wv.vocab)):\n",
    "        embedding_vector = model.wv[model.wv.index2word[i]]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = []\n",
    "vector_dim = 100\n",
    "for i in df['docstring2']:\n",
    "    #print(i[0])\n",
    " #   print(i)\n",
    "    if len(i[0]) ==0:\n",
    "        embed.append([[[0]]])\n",
    "    else:\n",
    "        model = Word2Vec(i, sg=1,iter=10, min_count=1, size=100, workers=4)\n",
    "        embedding_matrix = create_embedding_matrix(model)\n",
    "        embed.append([embedding_matrix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['doc_string_embed']=embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "      <th>docstrings</th>\n",
       "      <th>docstring2</th>\n",
       "      <th>doc_string_embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ilogue/niprov tests/test_mediumviewer.py</td>\n",
       "      <td>#!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\nfr...</td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[[0]]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UndeadMastodon/Loltris Matrix.py</td>\n",
       "      <td>#!/usr/bin/python2\\n#-*- coding: utf-8 -*-\\n\\n...</td>\n",
       "      <td>prints a matrix to the console for debugging p...</td>\n",
       "      <td>[[prints, a, matrix, to, the, console, for, de...</td>\n",
       "      <td>[[[-0.0015476032858714461, 0.00098642380908131...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anhstudios/swganh data/scripts/templates/objec...</td>\n",
       "      <td>#### NOTICE: THIS FILE IS AUTOGENERATED\\n#### ...</td>\n",
       "      <td></td>\n",
       "      <td>[[]]</td>\n",
       "      <td>[[[0]]]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_path  \\\n",
       "0           ilogue/niprov tests/test_mediumviewer.py   \n",
       "1                   UndeadMastodon/Loltris Matrix.py   \n",
       "2  anhstudios/swganh data/scripts/templates/objec...   \n",
       "\n",
       "                                             content  \\\n",
       "0  #!/usr/bin/python\\n# -*- coding: UTF-8 -*-\\nfr...   \n",
       "1  #!/usr/bin/python2\\n#-*- coding: utf-8 -*-\\n\\n...   \n",
       "2  #### NOTICE: THIS FILE IS AUTOGENERATED\\n#### ...   \n",
       "\n",
       "                                          docstrings  \\\n",
       "0                                                      \n",
       "1  prints a matrix to the console for debugging p...   \n",
       "2                                                      \n",
       "\n",
       "                                          docstring2  \\\n",
       "0                                               [[]]   \n",
       "1  [[prints, a, matrix, to, the, console, for, de...   \n",
       "2                                               [[]]   \n",
       "\n",
       "                                    doc_string_embed  \n",
       "0                                            [[[0]]]  \n",
       "1  [[[-0.0015476032858714461, 0.00098642380908131...  \n",
       "2                                            [[[0]]]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_search(words):\n",
    "    words_l = words.split()\n",
    "    model = Word2Vec(sentences, sg=1, min_count=1, size=100, workers=4)\n",
    "    embedding_matrix = create_embedding_matrix(model)\n",
    "    return (embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nn(words, embeddings):\n",
    "    \n",
    "    search = embed_search(words)\n",
    "    distances = [scipy.spatial.distance.cosine(search[0], i[0][0][0]) for i in embeddings]\n",
    "    nn = np.argsort(np.asarray(distances))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cathyzhou/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['# -*- coding: utf-8 -*-\\r\\n\"\"\"\\r\\n/***************************************************************************\\r\\n    Useful network functions\\r\\n                             -------------------\\r\\n    begin            : 2011-03-01\\r\\n    copyright        : (C) 2011 by Luiz Motta\\r\\n    author           : Luiz P. Motta\\r\\n    email            : motta _dot_ luiz _at_ gmail.com\\r\\n ***************************************************************************/\\r\\n\\r\\n/***************************************************************************\\r\\n *                                                                         *\\r\\n *   This program is free software; you can redistribute it and/or modify  *\\r\\n *   it under the terms of the GNU General Public License as published by  *\\r\\n *   the Free Software Foundation; either version 2 of the License, or     *\\r\\n *   (at your option) any later version.                                   *\\r\\n *                                                                         *\\r\\n ***************************************************************************/\\r\\n\"\"\"\\r\\nfrom PyQt4.QtCore import *\\r\\nfrom PyQt4.QtNetwork import *\\r\\n\\r\\ndef getProxy():\\r\\n    \\r\\n    # Adaption by source of \"Plugin Installer - Version 1.0.10\" \\r\\n    proxy = None\\r\\n    settings = QSettings()\\r\\n    settings.beginGroup(\"proxy\")\\r\\n    \\r\\n    #Check if the \\'proxy\\' group exists\\r\\n    proxyKeys = settings.childKeys()\\r\\n    if len(proxyKeys) == 0:\\r\\n        return\\r\\n    \\r\\n    if settings.value(\"/proxyEnabled\",type=bool):\\r\\n        proxy = QNetworkProxy()\\r\\n        proxyType = settings.value(\"/proxyType\",\"\")\\r\\n        #if len(args)>0 and settings.value(\"/proxyExcludedUrls\").toString().contains(args[0]):\\r\\n        #  proxyType = \"NoProxy\"\\r\\n        if proxyType in [\"1\",\"Socks5Proxy\"]:\\r\\n            proxy.setType(QNetworkProxy.Socks5Proxy)\\r\\n        elif proxyType in [\"2\",\"NoProxy\"]:\\r\\n            proxy.setType(QNetworkProxy.NoProxy)\\r\\n        elif proxyType in [\"3\",\"HttpProxy\"]:\\r\\n            proxy.setType(QNetworkProxy.HttpProxy)\\r\\n        elif proxyType in [\"4\",\"HttpCachingProxy\"] and QT_VERSION >= 0X040400:\\r\\n            proxy.setType(QNetworkProxy.HttpCachingProxy)\\r\\n        elif proxyType in [\"5\",\"FtpCachingProxy\"] and QT_VERSION >= 0X040400:\\r\\n            proxy.setType(QNetworkProxy.FtpCachingProxy)\\r\\n        else: \\r\\n            proxy.setType(QNetworkProxy.DefaultProxy)\\r\\n            proxy.setHostName(settings.value(\"/proxyHost\"))\\r\\n            port = settings.value(\"/proxyPort\")\\r\\n            if port != \"\":\\r\\n                proxy.setPort(int(port))\\r\\n            proxy.setUser(settings.value(\"/proxyUser\"))\\r\\n            proxy.setPassword(settings.value(\"/proxyPassword\"))\\r\\n    \\r\\n    settings.endGroup()\\r\\n    return proxy', 'from os import environ, path\\nfrom pyiso import client_factory\\nfrom unittest import TestCase\\nfrom datetime import datetime\\nimport requests_mock\\nimport pytz\\n\\n\\ndef read_fixture(filename):\\n    fixtures_base_path = path.join(path.dirname(__file__), \\'../fixtures/isone\\')\\n    return open(path.join(fixtures_base_path, filename), \\'r\\').read().encode(\\'utf-8\\')\\n\\n\\nclass TestISONE(TestCase):\\n    def setUp(self):\\n        environ[\\'ISONE_USERNAME\\'] = \\'test\\'\\n        environ[\\'ISONE_PASSWORD\\'] = \\'test\\'\\n        self.c = client_factory(\\'ISONE\\')\\n\\n    def test_auth(self):\\n        \"\"\"Auth info should be set up from env during init\"\"\"\\n        self.assertEqual(len(self.c.auth), 2)\\n\\n    def test_utcify(self):\\n        ts_str = \\'2014-05-03T02:32:44.000-04:00\\'\\n        ts = self.c.utcify(ts_str)\\n        self.assertEqual(ts.year, 2014)\\n        self.assertEqual(ts.month, 5)\\n        self.assertEqual(ts.day, 3)\\n        self.assertEqual(ts.hour, 2+4)\\n        self.assertEqual(ts.minute, 32)\\n        self.assertEqual(ts.second, 44)\\n        self.assertEqual(ts.tzinfo, pytz.utc)\\n\\n    def test_handle_options_gen_latest(self):\\n        self.c.handle_options(data=\\'gen\\', latest=True)\\n        self.assertEqual(self.c.options[\\'market\\'], self.c.MARKET_CHOICES.na)\\n        self.assertEqual(self.c.options[\\'frequency\\'], self.c.FREQUENCY_CHOICES.na)\\n\\n    def test_handle_options_load_latest(self):\\n        self.c.handle_options(data=\\'load\\', latest=True)\\n        self.assertEqual(self.c.options[\\'market\\'], self.c.MARKET_CHOICES.fivemin)\\n        self.assertEqual(self.c.options[\\'frequency\\'], self.c.FREQUENCY_CHOICES.fivemin)\\n\\n    def test_handle_options_load_forecast(self):\\n        self.c.handle_options(data=\\'load\\', forecast=True)\\n        self.assertEqual(self.c.options[\\'market\\'], self.c.MARKET_CHOICES.dam)\\n        self.assertEqual(self.c.options[\\'frequency\\'], self.c.FREQUENCY_CHOICES.dam)\\n\\n    def test_endpoints_gen_latest(self):\\n        self.c.handle_options(data=\\'gen\\', latest=True)\\n        endpoints = self.c.request_endpoints()\\n        self.assertEqual(len(endpoints), 1)\\n        self.assertIn(\\'/genfuelmix/current.json\\', endpoints)\\n\\n    def test_endpoints_load_latest(self):\\n        self.c.handle_options(data=\\'load\\', latest=True)\\n        endpoints = self.c.request_endpoints()\\n        self.assertEqual(len(endpoints), 1)\\n        self.assertIn(\\'/fiveminutesystemload/current.json\\', endpoints)\\n\\n    def test_endpoints_gen_range(self):\\n        self.c.handle_options(data=\\'gen\\',\\n                              start_at=pytz.utc.localize(datetime(2016, 5, 2, 12)),\\n                              end_at=pytz.utc.localize(datetime(2016, 5, 2, 14)))\\n        endpoints = self.c.request_endpoints()\\n        self.assertEqual(len(endpoints), 1)\\n        self.assertIn(\\'/genfuelmix/day/20160502.json\\', endpoints)\\n\\n    def test_endpoints_load_range(self):\\n        self.c.handle_options(data=\\'load\\',\\n                              start_at=pytz.utc.localize(datetime(2016, 5, 2, 12)),\\n                              end_at=pytz.utc.localize(datetime(2016, 5, 2, 14)))\\n        endpoints = self.c.request_endpoints()\\n        self.assertEqual(len(endpoints), 1)\\n        self.assertIn(\\'/fiveminutesystemload/day/20160502.json\\', endpoints)\\n\\n    def test_endpoints_load_forecast(self):\\n        self.c.handle_options(data=\\'load\\', forecast=True)\\n        endpoints = self.c.request_endpoints()\\n        self.assertGreaterEqual(len(endpoints), 3)\\n        self.assertIn(\\'/hourlyloadforecast/day/\\', endpoints[0])\\n\\n        now = pytz.utc.localize(datetime.utcnow()).astimezone(pytz.timezone(\\'US/Eastern\\'))\\n        date_str = now.strftime(\\'%Y%m%d\\')\\n        self.assertIn(date_str, endpoints[0])\\n\\n    @requests_mock.Mocker()\\n    def test_get_morningreport(self, mock_request):\\n        expected_url = \\'https://webservices.iso-ne.com/api/v1.1/morningreport/current.json\\'\\n        expected_response = read_fixture(\\'morningreport_current.json\\')\\n        mock_request.get(expected_url, content=expected_response)\\n\\n        resp = self.c.get_morningreport()\\n        assert \"MorningReports\" in resp\\n\\n    @requests_mock.Mocker()\\n    def test_get_morningreport_for_day(self, mock_request):\\n        expected_url = \\'https://webservices.iso-ne.com/api/v1.1/morningreport/day/20160101.json\\'\\n        expected_response = read_fixture(\\'morningreport_20160101.json\\')\\n        mock_request.get(expected_url, content=expected_response)\\n\\n        resp = self.c.get_morningreport(day=\"20160101\")\\n        assert resp[\"MorningReports\"][\"MorningReport\"][0][\"BeginDate\"] == \"2016-01-01T00:00:00.000-05:00\"\\n\\n    def test_get_morningreport_bad_date(self):\\n        self.assertRaises(ValueError, self.c.get_morningreport, day=\"foo\")\\n\\n    @requests_mock.Mocker()\\n    def test_get_sevendayforecast(self, mock_request):\\n        expected_url = \\'https://webservices.iso-ne.com/api/v1.1/sevendayforecast/current.json\\'\\n        expected_response = read_fixture(\\'sevendayforecast_current.json\\')\\n        mock_request.get(expected_url, content=expected_response)\\n\\n        resp = self.c.get_sevendayforecast()\\n        assert \"SevenDayForecasts\" in resp\\n\\n    @requests_mock.Mocker()\\n    def test_get_sevendayforecast_for_day(self, mock_request):\\n        expected_url = \\'https://webservices.iso-ne.com/api/v1.1/sevendayforecast/day/20160101.json\\'\\n        expected_response = read_fixture(\\'sevendayforecast_20160101.json\\')\\n        mock_request.get(expected_url, content=expected_response)\\n\\n        resp = self.c.get_sevendayforecast(day=\"20160101\")\\n        assert resp[\"SevenDayForecasts\"][\"SevenDayForecast\"][0][\"BeginDate\"] == \"2016-01-01T00:00:00.000-05:00\"\\n\\n    def test_get_sevendayforecast_bad_date(self):\\n        self.assertRaises(ValueError, self.c.get_sevendayforecast, day=\"foo\")\\n', 'import errno\\nimport os\\nimport selectors\\nimport signal\\nimport socket\\nimport struct\\nimport sys\\nimport threading\\n\\nfrom . import connection\\nfrom . import process\\nfrom .context import reduction\\nfrom . import semaphore_tracker\\nfrom . import spawn\\nfrom . import util\\n\\n__all__ = [\\'ensure_running\\', \\'get_inherited_fds\\', \\'connect_to_new_process\\',\\n           \\'set_forkserver_preload\\']\\n\\n#\\n#\\n#\\n\\nMAXFDS_TO_SEND = 256\\nUNSIGNED_STRUCT = struct.Struct(\\'Q\\')     # large enough for pid_t\\n\\n#\\n# Forkserver class\\n#\\n\\nclass ForkServer(object):\\n\\n    def __init__(self):\\n        self._forkserver_address = None\\n        self._forkserver_alive_fd = None\\n        self._inherited_fds = None\\n        self._lock = threading.Lock()\\n        self._preload_modules = [\\'__main__\\']\\n\\n    def set_forkserver_preload(self, modules_names):\\n        \\'\\'\\'Set list of module names to try to load in forkserver process.\\'\\'\\'\\n        if not all(type(mod) is str for mod in self._preload_modules):\\n            raise TypeError(\\'module_names must be a list of strings\\')\\n        self._preload_modules = modules_names\\n\\n    def get_inherited_fds(self):\\n        \\'\\'\\'Return list of fds inherited from parent process.\\n\\n        This returns None if the current process was not started by fork\\n        server.\\n        \\'\\'\\'\\n        return self._inherited_fds\\n\\n    def connect_to_new_process(self, fds):\\n        \\'\\'\\'Request forkserver to create a child process.\\n\\n        Returns a pair of fds (status_r, data_w).  The calling process can read\\n        the child process\\'s pid and (eventually) its returncode from status_r.\\n        The calling process should write to data_w the pickled preparation and\\n        process data.\\n        \\'\\'\\'\\n        self.ensure_running()\\n        if len(fds) + 4 >= MAXFDS_TO_SEND:\\n            raise ValueError(\\'too many fds\\')\\n        with socket.socket(socket.AF_UNIX) as client:\\n            client.connect(self._forkserver_address)\\n            parent_r, child_w = os.pipe()\\n            child_r, parent_w = os.pipe()\\n            allfds = [child_r, child_w, self._forkserver_alive_fd,\\n                      semaphore_tracker.getfd()]\\n            allfds += fds\\n            try:\\n                reduction.sendfds(client, allfds)\\n                return parent_r, parent_w\\n            except:\\n                os.close(parent_r)\\n                os.close(parent_w)\\n                raise\\n            finally:\\n                os.close(child_r)\\n                os.close(child_w)\\n\\n    def ensure_running(self):\\n        \\'\\'\\'Make sure that a fork server is running.\\n\\n        This can be called from any process.  Note that usually a child\\n        process will just reuse the forkserver started by its parent, so\\n        ensure_running() will do nothing.\\n        \\'\\'\\'\\n        with self._lock:\\n            semaphore_tracker.ensure_running()\\n            if self._forkserver_alive_fd is not None:\\n                return\\n\\n            cmd = (\\'from multiprocessing.forkserver import main; \\' +\\n                   \\'main(%d, %d, %r, **%r)\\')\\n\\n            if self._preload_modules:\\n                desired_keys = {\\'main_path\\', \\'sys_path\\'}\\n                data = spawn.get_preparation_data(\\'ignore\\')\\n                data = dict((x,y) for (x,y) in data.items()\\n                            if x in desired_keys)\\n            else:\\n                data = {}\\n\\n            with socket.socket(socket.AF_UNIX) as listener:\\n                address = connection.arbitrary_address(\\'AF_UNIX\\')\\n                listener.bind(address)\\n                os.chmod(address, 0o600)\\n                listener.listen()\\n\\n                # all client processes own the write end of the \"alive\" pipe;\\n                # when they all terminate the read end becomes ready.\\n                alive_r, alive_w = os.pipe()\\n                try:\\n                    fds_to_pass = [listener.fileno(), alive_r]\\n                    cmd %= (listener.fileno(), alive_r, self._preload_modules,\\n                            data)\\n                    exe = spawn.get_executable()\\n                    args = [exe] + util._args_from_interpreter_flags()\\n                    args += [\\'-c\\', cmd]\\n                    pid = util.spawnv_passfds(exe, args, fds_to_pass)\\n                except:\\n                    os.close(alive_w)\\n                    raise\\n                finally:\\n                    os.close(alive_r)\\n                self._forkserver_address = address\\n                self._forkserver_alive_fd = alive_w\\n\\n#\\n#\\n#\\n\\ndef main(listener_fd, alive_r, preload, main_path=None, sys_path=None):\\n    \\'\\'\\'Run forkserver.\\'\\'\\'\\n    if preload:\\n        if \\'__main__\\' in preload and main_path is not None:\\n            process.current_process()._inheriting = True\\n            try:\\n                spawn.import_main_path(main_path)\\n            finally:\\n                del process.current_process()._inheriting\\n        for modname in preload:\\n            try:\\n                __import__(modname)\\n            except ImportError:\\n                pass\\n\\n    util._close_stdin()\\n\\n    # ignoring SIGCHLD means no need to reap zombie processes\\n    handler = signal.signal(signal.SIGCHLD, signal.SIG_IGN)\\n    with socket.socket(socket.AF_UNIX, fileno=listener_fd) as listener, \\\\\\n         selectors.DefaultSelector() as selector:\\n        _forkserver._forkserver_address = listener.getsockname()\\n\\n        selector.register(listener, selectors.EVENT_READ)\\n        selector.register(alive_r, selectors.EVENT_READ)\\n\\n        while True:\\n            try:\\n                while True:\\n                    rfds = [key.fileobj for (key, events) in selector.select()]\\n                    if rfds:\\n                        break\\n\\n                if alive_r in rfds:\\n                    # EOF because no more client processes left\\n                    assert os.read(alive_r, 1) == b\\'\\'\\n                    raise SystemExit\\n\\n                assert listener in rfds\\n                with listener.accept()[0] as s:\\n                    code = 1\\n                    if os.fork() == 0:\\n                        try:\\n                            _serve_one(s, listener, alive_r, handler)\\n                        except Exception:\\n                            sys.excepthook(*sys.exc_info())\\n                            sys.stderr.flush()\\n                        finally:\\n                            os._exit(code)\\n\\n            except OSError as e:\\n                if e.errno != errno.ECONNABORTED:\\n                    raise\\n\\ndef _serve_one(s, listener, alive_r, handler):\\n    # close unnecessary stuff and reset SIGCHLD handler\\n    listener.close()\\n    os.close(alive_r)\\n    signal.signal(signal.SIGCHLD, handler)\\n\\n    # receive fds from parent process\\n    fds = reduction.recvfds(s, MAXFDS_TO_SEND + 1)\\n    s.close()\\n    assert len(fds) <= MAXFDS_TO_SEND\\n    (child_r, child_w, _forkserver._forkserver_alive_fd,\\n     stfd, *_forkserver._inherited_fds) = fds\\n    semaphore_tracker._semaphore_tracker._fd = stfd\\n\\n    # send pid to client processes\\n    write_unsigned(child_w, os.getpid())\\n\\n    # reseed random number generator\\n    if \\'random\\' in sys.modules:\\n        import random\\n        random.seed()\\n\\n    # run process object received over pipe\\n    code = spawn._main(child_r)\\n\\n    # write the exit code to the pipe\\n    write_unsigned(child_w, code)\\n\\n#\\n# Read and write unsigned numbers\\n#\\n\\ndef read_unsigned(fd):\\n    data = b\\'\\'\\n    length = UNSIGNED_STRUCT.size\\n    while len(data) < length:\\n        s = os.read(fd, length - len(data))\\n        if not s:\\n            raise EOFError(\\'unexpected EOF\\')\\n        data += s\\n    return UNSIGNED_STRUCT.unpack(data)[0]\\n\\ndef write_unsigned(fd, n):\\n    msg = UNSIGNED_STRUCT.pack(n)\\n    while msg:\\n        nbytes = os.write(fd, msg)\\n        if nbytes == 0:\\n            raise RuntimeError(\\'should not get here\\')\\n        msg = msg[nbytes:]\\n\\n#\\n#\\n#\\n\\n_forkserver = ForkServer()\\nensure_running = _forkserver.ensure_running\\nget_inherited_fds = _forkserver.get_inherited_fds\\nconnect_to_new_process = _forkserver.connect_to_new_process\\nset_forkserver_preload = _forkserver.set_forkserver_preload\\n', '\\'\\'\\'\\ncpress - python bindings\\n\\n-------------------------------------------------------------------------------\\n\\nInstallation:\\n\\n    python2 setup.py install\\n\\nUsage:\\n\\nImport module with:\\n\\n    from cpress import *\\n\\nSee self-explanatory file `examples/example.py` for more informations.\\n\\n-------------------------------------------------------------------------------\\n\\nLicense: MIT (http://www.opensource.org/licenses/mit-license.php)\\nRepository: https://github.com/solusipse/cpress\\n\\n-------------------------------------------------------------------------------\\n\\'\\'\\'\\n\\nfrom ctypes import cdll\\nfrom keylist import *\\n\\nimport distutils.sysconfig as sysconfig\\nfrom os import path\\n\\n_lpath = path.abspath(path.join(path.dirname(__file__), \"..\", \"_cpress.so\"))\\n_cpress = cdll.LoadLibrary(_lpath)\\n\\nclass Cpress:\\n\\n    def __init__(self):\\n        _cpress.initialize()\\n\\n    def close(self):\\n        _cpress.finish()\\n\\n    def press_key(self, key):\\n        _cpress.press_key(key)\\n\\n    def hold_key(self, key):\\n        _cpress.hold_key(key)\\n\\n    def release_key(self, key):\\n        _cpress.release_key(key)\\n\\n    def press_combination(self, keys):\\n        _cpress.press_combination(len(keys)-1, *keys)\\n', '#!/usr/bin/python3\\n# Copyright (C) 2013 Kristoffer Gronlund <kgronlund@suse.com>\\n# See COPYING for license information.\\n\\nimport sys\\nimport json\\nimport subprocess\\n\\n\\ndef run(cmd):\\n    proc = subprocess.Popen(cmd,\\n                            shell=False,\\n                            stdin=None,\\n                            stdout=subprocess.PIPE,\\n                            stderr=subprocess.PIPE)\\n    out, err = proc.communicate(None)\\n    proc.wait()\\n    return proc.returncode, out.decode(\\'utf-8\\'), err.decode(\\'utf-8\\')\\n\\n\\ndef package_data(pkg):\\n    \"\"\"\\n    Gathers version and release information about a package.\\n    \"\"\"\\n    _qfmt = \\'version: %{VERSION}\\\\nrelease: %{RELEASE}\\\\n\\'\\n    rc, out, err = run([\\'/bin/rpm\\', \\'-q\\', \\'--queryformat=\\' + _qfmt, pkg])\\n    if rc == 0:\\n        data = {\\'name\\': pkg}\\n        for line in out.split(\\'\\\\n\\'):\\n            info = line.split(\\':\\', 1)\\n            if len(info) == 2:\\n                data[info[0].strip()] = info[1].strip()\\n        return data\\n    else:\\n        return {\\'name\\': pkg, \\'error\\': \"package not installed\"}\\n\\n\\ndef main():\\n    data = [package_data(pkg) for pkg in sys.argv[1:]]\\n    print(json.dumps(data))\\n\\nmain()\\n']\n"
     ]
    }
   ],
   "source": [
    "def top_n_code(search_terms, docstrings, embeddings, n):\n",
    "    \n",
    "    top_n = find_nn(search_terms, embeddings)[0:n]\n",
    "    code = [df['content'].iloc[i] for i in top_n]\n",
    "    \n",
    "    return code\n",
    "    #return top_n\n",
    "    \n",
    "search = \"model for LSTM network\"\n",
    "doc_strings = list(df['docstrings'])\n",
    "embed_vecs = list(df['doc_string_embed'])\n",
    "\n",
    "print(top_n_code(search, doc_strings, embed_vecs, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
