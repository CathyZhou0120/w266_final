{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tools import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, you will need to CMD and authenticate with \n",
    "\n",
    "'gcloud auth application-default login'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project='manifest-frame-203601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"\"\"\n",
    "    select * from w266_final.final_20k\n",
    "    LIMIT 20000\"\"\")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "df = []\n",
    "for row in rows:\n",
    "    df.append([row.repo_path,row.c_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>repo_path</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>napalm-automation/napalm-yang napalm_yang/mode...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\nfrom operator import ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>napalm-automation/napalm-yang napalm_yang/mode...</td>\n",
       "      <td># -*- coding: utf-8 -*-\\nfrom operator import ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hq6/smux smux.py</td>\n",
       "      <td>#!/usr/bin/env python\\n\\n# Copyright (c) 2014-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lloda/ra-ra ra.py</td>\n",
       "      <td># -*- mode: Python -*-\\n# -*- coding: utf-8 -*...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dcos/dcos setup.py</td>\n",
       "      <td>from setuptools import setup\\n\\n\\ndef get_adva...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           repo_path  \\\n",
       "0  napalm-automation/napalm-yang napalm_yang/mode...   \n",
       "1  napalm-automation/napalm-yang napalm_yang/mode...   \n",
       "2                                   hq6/smux smux.py   \n",
       "3                                  lloda/ra-ra ra.py   \n",
       "4                                 dcos/dcos setup.py   \n",
       "\n",
       "                                             content  \n",
       "0  # -*- coding: utf-8 -*-\\nfrom operator import ...  \n",
       "1  # -*- coding: utf-8 -*-\\nfrom operator import ...  \n",
       "2  #!/usr/bin/env python\\n\\n# Copyright (c) 2014-...  \n",
       "3  # -*- mode: Python -*-\\n# -*- coding: utf-8 -*...  \n",
       "4  from setuptools import setup\\n\\n\\ndef get_adva...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['repo_path','content']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(docstring_list):\n",
    "    \n",
    "    \"\"\"takes a list of doc strings and converts to a single flat list of tokens\"\"\"\n",
    "    \n",
    "    tokens = [tf.keras.preprocessing.text.text_to_word_sequence(i) for i in docstring_list]\n",
    "    flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "    flat_string = \" \".join(flat_tokens)\n",
    "    \n",
    "    return flat_string\n",
    "\n",
    "def get_docstrings(source):\n",
    "    \n",
    "    \"\"\"function to walk through parse tree and return list of docstrings\"\"\"\n",
    "    \n",
    "    NODE_TYPES = {\n",
    "    ast.ClassDef: 'Class',\n",
    "    ast.FunctionDef: 'Function/Method',\n",
    "    ast.Module: 'Module'\n",
    "    }\n",
    "    \n",
    "    docstrings = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(source)\n",
    "    except:\n",
    "        return \" \"\n",
    "       \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, tuple(NODE_TYPES)):\n",
    "            docstring = ast.get_docstring(node)\n",
    "            docstrings.append(docstring)\n",
    "    \n",
    "    docstrings =  [x for x in docstrings if x is not None]\n",
    "    clean_string = cleanup(docstrings)\n",
    "            \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docstrings'] = [get_docstrings(x) for x in list(df['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up corpus for count vectorizer\n",
    "corpus = list(df['docstrings'])\n",
    "\n",
    "#count values for tfidf calculations\n",
    "count_vect = CountVectorizer()\n",
    "count_vect = count_vect.fit(corpus)\n",
    "freq_term_matrix = count_vect.transform(corpus)\n",
    "\n",
    "#to grab columns for words\n",
    "vocab = count_vect.vocabulary_\n",
    "\n",
    "#create a holder for the new df column\n",
    "embeddings_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embed(words):\n",
    "    \n",
    "    global count_vect, freq_term_matrix, vocab\n",
    "    \n",
    "    #verify there are docstrings available\n",
    "    if len(words)==0:\n",
    "        return np.zeros(100)\n",
    "         \n",
    "    #create tfidf for each document\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tfidf.fit(freq_term_matrix)\n",
    "    doc_freq_term = count_vect.transform([words])\n",
    "    idfs = tfidf.transform(doc_freq_term)\n",
    "\n",
    "    #split the docstrings to individual words for average\n",
    "    sent_list = words.split(\" \")\n",
    "    embeddings = []\n",
    "\n",
    "    #cycle through list of words in docstring\n",
    "    for i in range(len(sent_list)):\n",
    "\n",
    "        if sent_list[i] in vocab:\n",
    "\n",
    "            col = vocab[sent_list[i]]\n",
    "            embed = hands.get_vector(sent_list[i], strict=False)\n",
    "            tfidf = idfs[0, col]\n",
    "            embeddings.append(np.multiply(embed, tfidf))\n",
    "\n",
    "        embed_array = np.asarray(embeddings)\n",
    "        \n",
    "        if len(embed_array)==0:\n",
    "            return np.zeros(100)\n",
    "\n",
    "        return np.mean(embed_array, axis=0)\n",
    "    \n",
    "def find_nn(words, embeddings):\n",
    "    \n",
    "    search = words_to_embed(words)\n",
    "    distances = [scipy.spatial.distance.cosine(search, i) for i in embeddings]\n",
    "    nn = np.argsort(np.asarray(distances))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = [words_to_embed(x) for x in list(df['docstrings'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_code(search_terms, docstrings, embeddings, n):\n",
    "    \n",
    "    top_n = find_nn(search_terms, embeddings)[0:n]\n",
    "    code = [df['content'][i] for i in top_n]\n",
    "    \n",
    "    return code\n",
    "\n",
    "doc_strings = list(df['docstrings'])\n",
    "embed_vecs = list(df['embeddings'])\n",
    "\n",
    "def make_query_file(query, results, filename):\n",
    "    \n",
    "    output = open(filename, 'w')\n",
    "    for item in results:\n",
    "        output.write(\"Query: \"+query+'\\n')\n",
    "        output.write(\"\\n************************** NEXT RESULT **************************************\\n\")\n",
    "        output.write(\"%s\\n\" % item)\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search1 = \"function that calculates distance\"\n",
    "query1 = top_n_code(search1, doc_strings, embed_vecs, 10)\n",
    "filename = 'model_1_queries/mod1_q1.txt'\n",
    "\n",
    "make_query_file(search1, query1, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "search2 = \"code to merge two files\"\n",
    "query2 = top_n_code(search2, doc_strings, embed_vecs, 10)\n",
    "filename = 'model_1_queries/mod1_q2.txt'\n",
    "\n",
    "make_query_file(search2, query2, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "search3 = \"train a neural network for image reconition\"\n",
    "query3 = top_n_code(search3, doc_strings, embed_vecs, 10)\n",
    "filename = 'model_1_queries/mod1_q3.txt'\n",
    "\n",
    "make_query_file(search3, query3, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Rekall Memory Forensics\n",
      "# Copyright (C) 2007-2011 Volatile Systems\n",
      "# Copyright 2013 Google Inc. All Rights Reserved.\n",
      "#\n",
      "# Additional Authors:\n",
      "# Michael Cohen <scudette@users.sourceforge.net>\n",
      "# Mike Auty <mike.auty@gmail.com>\n",
      "#\n",
      "# This program is free software; you can redistribute it and/or modify\n",
      "# it under the terms of the GNU General Public License as published by\n",
      "# the Free Software Foundation; either version 2 of the License, or (at\n",
      "# your option) any later version.\n",
      "#\n",
      "# This program is distributed in the hope that it will be useful, but\n",
      "# WITHOUT ANY WARRANTY; without even the implied warranty of\n",
      "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n",
      "# General Public License for more details.\n",
      "#\n",
      "# You should have received a copy of the GNU General Public License\n",
      "# along with this program; if not, write to the Free Software\n",
      "# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA\n",
      "#\n",
      "\n",
      "# pylint: disable=protected-access\n",
      "\n",
      "from future import standard_library\n",
      "standard_library.install_aliases()\n",
      "from rekall import testlib\n",
      "from rekall_lib import utils\n",
      "\n",
      "from rekall.plugins.common import memmap\n",
      "from rekall.plugins.windows import common\n",
      "\n",
      "\n",
      "class WinPsList(common.WinProcessFilter):\n",
      "    \"\"\"List processes for windows.\"\"\"\n",
      "\n",
      "    __name = \"pslist\"\n",
      "\n",
      "    eprocess = None\n",
      "\n",
      "    table_header = [\n",
      "        dict(type=\"_EPROCESS\", name=\"_EPROCESS\"),\n",
      "        dict(name=\"ppid\", width=6, align=\"r\"),\n",
      "        dict(name=\"thread_count\", width=6, align=\"r\"),\n",
      "        dict(name=\"handle_count\", width=8, align=\"r\"),\n",
      "        dict(name=\"session_id\", width=6, align=\"r\"),\n",
      "        dict(name=\"wow64\", width=6),\n",
      "        dict(name=\"process_create_time\", width=24),\n",
      "        dict(name=\"process_exit_time\", width=24)\n",
      "    ]\n",
      "\n",
      "    def column_types(self):\n",
      "        result = self._row(self.session.profile._EPROCESS())\n",
      "        result[\"handle_count\"] = result[\"ppid\"]\n",
      "        result[\"session_id\"] = result[\"ppid\"]\n",
      "\n",
      "        return result\n",
      "\n",
      "    def _row(self, task):\n",
      "        return dict(_EPROCESS=task,\n",
      "                    ppid=task.InheritedFromUniqueProcessId,\n",
      "                    thread_count=task.ActiveThreads,\n",
      "                    handle_count=task.ObjectTable.m(\"HandleCount\"),\n",
      "                    session_id=task.SessionId,\n",
      "                    wow64=task.IsWow64,\n",
      "                    process_create_time=task.CreateTime,\n",
      "                    process_exit_time=task.ExitTime)\n",
      "\n",
      "    def collect(self):\n",
      "        for task in self.filter_processes():\n",
      "            yield self._row(task)\n",
      "\n",
      "\n",
      "class WinDllList(common.WinProcessFilter):\n",
      "    \"\"\"Prints a list of dll modules mapped into each process.\"\"\"\n",
      "\n",
      "    __name = \"dlllist\"\n",
      "\n",
      "    table_header = [\n",
      "        dict(name=\"divider\", type=\"Divider\"),\n",
      "        dict(name=\"_EPROCESS\", hidden=True),\n",
      "        dict(name=\"base\", style=\"address\"),\n",
      "        dict(name=\"size\", style=\"address\"),\n",
      "        dict(name=\"reason\", width=30),\n",
      "        dict(name=\"dll_path\"),\n",
      "    ]\n",
      "\n",
      "    def collect(self):\n",
      "        for task in self.filter_processes():\n",
      "            pid = task.UniqueProcessId\n",
      "\n",
      "            divider = \"{0} pid: {1:6}\\n\".format(task.ImageFileName, pid)\n",
      "\n",
      "            if task.Peb:\n",
      "                divider += u\"Command line : {0}\\n\".format(\n",
      "                    task.Peb.ProcessParameters.CommandLine)\n",
      "\n",
      "                divider += u\"{0}\\n\\n\".format(task.Peb.CSDVersion)\n",
      "                yield dict(divider=divider)\n",
      "\n",
      "                for m in task.get_load_modules():\n",
      "                    yield dict(base=m.DllBase,\n",
      "                               size=m.SizeOfImage,\n",
      "                               reason=m.LoadReason,\n",
      "                               dll_path=m.FullDllName,\n",
      "                               _EPROCESS=task)\n",
      "            else:\n",
      "                yield dict(divider=\"Unable to read PEB for task.\\n\")\n",
      "\n",
      "\n",
      "class WinMemMap(memmap.MemmapMixIn, common.WinProcessFilter):\n",
      "    \"\"\"Calculates the memory regions mapped by a process.\"\"\"\n",
      "    __name = \"memmap\"\n",
      "\n",
      "    def _get_highest_user_address(self):\n",
      "        return self.profile.get_constant_object(\n",
      "            \"MmHighestUserAddress\", \"Pointer\").v()\n",
      "\n",
      "\n",
      "class Threads(common.WinProcessFilter):\n",
      "    \"\"\"Enumerate threads.\"\"\"\n",
      "    name = \"threads\"\n",
      "\n",
      "    table_header = [\n",
      "        dict(name=\"_ETHREAD\", style=\"address\"),\n",
      "        dict(name=\"pid\", align=\"r\", width=6),\n",
      "        dict(name=\"tid\", align=\"r\", width=6),\n",
      "        dict(name=\"start\", style=\"address\"),\n",
      "        dict(name=\"start_symbol\", width=30),\n",
      "        dict(name=\"Process\", width=16),\n",
      "        dict(name=\"win32_start\", style=\"address\"),\n",
      "        dict(name=\"win32_start_symb\")\n",
      "    ]\n",
      "\n",
      "    def collect(self):\n",
      "        cc = self.session.plugins.cc()\n",
      "        with cc:\n",
      "            for task in self.filter_processes():\n",
      "                # Resolve names in the process context.\n",
      "                cc.SwitchProcessContext(process=task)\n",
      "\n",
      "                for thread in task.ThreadListHead.list_of_type(\n",
      "                        \"_ETHREAD\", \"ThreadListEntry\"):\n",
      "\n",
      "                    yield dict(_ETHREAD=thread,\n",
      "                               pid=thread.Cid.UniqueProcess,\n",
      "                               tid=thread.Cid.UniqueThread,\n",
      "                               start=thread.StartAddress,\n",
      "                               start_symbol=utils.FormattedAddress(\n",
      "                                   self.session.address_resolver,\n",
      "                                   thread.StartAddress),\n",
      "                               Process=task.ImageFileName,\n",
      "                               win32_start=thread.Win32StartAddress,\n",
      "                               win32_start_symb=utils.FormattedAddress(\n",
      "                                   self.session.address_resolver,\n",
      "                                   thread.Win32StartAddress,\n",
      "                                   ))\n",
      "\n",
      "\n",
      "class WinMemDump(memmap.MemDumpMixin, common.WinProcessFilter):\n",
      "    \"\"\"Dump windows processes.\"\"\"\n",
      "\n",
      "\n",
      "class TestWinMemDump(testlib.HashChecker):\n",
      "    \"\"\"Test the pslist module.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(\n",
      "        commandline=\"memdump %(pids)s --dump_dir %(tempdir)s\",\n",
      "        pid=2624)\n",
      "\n",
      "\n",
      "class TestMemmap(testlib.SimpleTestCase):\n",
      "    \"\"\"Test the pslist module.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(\n",
      "        commandline=\"memmap %(pids)s\",\n",
      "        pid=2624)\n",
      "\n",
      "\n",
      "class TestMemmapCoalesce(testlib.SimpleTestCase):\n",
      "    \"\"\"Make sure that memmaps are coalesced properly.\"\"\"\n",
      "\n",
      "    PARAMETERS = dict(commandline=\"memmap %(pids)s --coalesce\",\n",
      "                      pid=2624)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search4 = \"list the first 100 Fibonacci Numbers\"\n",
    "query4 = top_n_code(search4, doc_strings, embed_vecs, 10)\n",
    "filename = 'model_1_queries/mod1_q4.txt'\n",
    "\n",
    "make_query_file(search3, query3, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#     Copyright 2018, Kay Hayen, mailto:kay.hayen@gmail.com\n",
      "#\n",
      "#     Part of \"Nuitka\", an optimizing Python compiler that is compatible and\n",
      "#     integrates with CPython, but also works on its own.\n",
      "#\n",
      "#     Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "#     you may not use this file except in compliance with the License.\n",
      "#     You may obtain a copy of the License at\n",
      "#\n",
      "#        http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "#     Unless required by applicable law or agreed to in writing, software\n",
      "#     distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "#     WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "#     See the License for the specific language governing permissions and\n",
      "#     limitations under the License.\n",
      "#\n",
      "\"\"\" Syntax highlighting for Python.\n",
      "\n",
      "Inspired/copied from by http://diotavelli.net/PyQtWiki/Python%20syntax%20highlighting\n",
      "\"\"\"\n",
      "\n",
      "from PyQt5.QtCore import (\n",
      "    QRegExp  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QColor  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QFont  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QSyntaxHighlighter  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "from PyQt5.QtGui import (\n",
      "    QTextCharFormat  # @UnresolvedImport pylint: disable=I0021,import-error\n",
      ")\n",
      "\n",
      "\n",
      "def createTextFormat(color, style = \"\"):\n",
      "    \"\"\"Return a QTextCharFormat with the given attributes.\n",
      "    \"\"\"\n",
      "    _color = QColor()\n",
      "    _color.setNamedColor(color)\n",
      "\n",
      "    _format = QTextCharFormat()\n",
      "    _format.setForeground(_color)\n",
      "    if \"bold\" in style:\n",
      "        _format.setFontWeight(QFont.Bold)\n",
      "    if \"italic\" in style:\n",
      "        _format.setFontItalic(True)\n",
      "\n",
      "    return _format\n",
      "\n",
      "\n",
      "# Syntax styles that can be shared by all languages\n",
      "STYLES = {\n",
      "    \"keyword\"  : createTextFormat(\"blue\"),\n",
      "    \"operator\" : createTextFormat(\"red\"),\n",
      "    \"brace\"    : createTextFormat(\"darkGray\"),\n",
      "    \"defclass\" : createTextFormat(\"black\", \"bold\"),\n",
      "    \"string\"   : createTextFormat(\"magenta\"),\n",
      "    \"string2\"  : createTextFormat(\"darkMagenta\"),\n",
      "    \"comment\"  : createTextFormat(\"darkGreen\", \"italic\"),\n",
      "    \"self\"     : createTextFormat(\"black\", \"italic\"),\n",
      "    \"numbers\"  : createTextFormat(\"brown\"),\n",
      "}\n",
      "\n",
      "\n",
      "class PythonHighlighter(QSyntaxHighlighter):\n",
      "    \"\"\" Syntax highlighter for the Python language.\n",
      "    \"\"\"\n",
      "    # Python keywords\n",
      "    keywords = [\n",
      "        \"and\", \"assert\", \"break\", \"class\", \"continue\", \"def\",\n",
      "        \"del\", \"elif\", \"else\", \"except\", \"exec\", \"finally\",\n",
      "        \"for\", \"from\", \"global\", \"if\", \"import\", \"in\",\n",
      "        \"is\", \"lambda\", \"not\", \"or\", \"pass\", \"print\",\n",
      "        \"raise\", \"return\", \"try\", \"while\", \"with\", \"yield\",\n",
      "        \"None\", \"True\", \"False\",\n",
      "    ]\n",
      "\n",
      "    # Python operators\n",
      "    operators = [\n",
      "        '=',\n",
      "        # Comparison\n",
      "        \"==\", \"!=\", '<', \"<=\", '>', \">=\",\n",
      "        # Arithmetic\n",
      "        \"\\+\", '-', \"\\*\", '/', \"//\", \"\\%\", \"\\*\\*\",\n",
      "        # In-place\n",
      "        \"\\+=\", \"-=\", \"\\*=\", \"/=\", \"\\%=\",\n",
      "        # Bitwise\n",
      "        \"\\^\", \"\\|\", \"\\&\", \"\\~\", \">>\", \"<<\",\n",
      "    ]\n",
      "\n",
      "    # Python braces\n",
      "    braces = [\n",
      "        \"\\{\", \"\\}\", \"\\(\", \"\\)\", \"\\[\", \"\\]\",\n",
      "    ]\n",
      "    def __init__(self, document):\n",
      "        QSyntaxHighlighter.__init__(self, document)\n",
      "\n",
      "        # Multi-line strings (expression, flag, style)\n",
      "        # The triple-quotes in these two lines will mess up the\n",
      "        # syntax highlighting from this point onward\n",
      "        self.tri_single = (QRegExp(\"'''\"), 1, STYLES[\"string2\"])\n",
      "        self.tri_double = (QRegExp('\"\"\"'), 2, STYLES[\"string2\"])\n",
      "\n",
      "        rules = []\n",
      "\n",
      "        # Keyword, operator, and brace rules\n",
      "        rules += [(r'\\b%s\\b' % w, 0, STYLES[\"keyword\"])\n",
      "            for w in PythonHighlighter.keywords]\n",
      "        rules += [(r'%s' % o, 0, STYLES[\"operator\"])\n",
      "            for o in PythonHighlighter.operators]\n",
      "        rules += [(r'%s' % b, 0, STYLES[\"brace\"])\n",
      "            for b in PythonHighlighter.braces]\n",
      "\n",
      "        # All other rules\n",
      "        rules += [\n",
      "            # 'self'\n",
      "            (r'\\bself\\b', 0, STYLES[\"self\"]),\n",
      "\n",
      "            # Double-quoted string, possibly containing escape sequences\n",
      "            (r'\"[^\"\\\\]*(\\\\.[^\"\\\\]*)*\"', 0, STYLES[\"string\"]),\n",
      "            # Single-quoted string, possibly containing escape sequences\n",
      "            (r\"'[^'\\\\]*(\\\\.[^'\\\\]*)*'\", 0, STYLES[\"string\"]),\n",
      "\n",
      "            # 'def' followed by an identifier\n",
      "            (r'\\bdef\\b\\s*(\\w+)', 1, STYLES[\"defclass\"]),\n",
      "            # 'class' followed by an identifier\n",
      "            (r'\\bclass\\b\\s*(\\w+)', 1, STYLES[\"defclass\"]),\n",
      "\n",
      "            # From '#' until a newline\n",
      "            (r'#[^\\n]*', 0, STYLES[\"comment\"]),\n",
      "\n",
      "            # Numeric literals\n",
      "            (r'\\b[+-]?[0-9]+[lL]?\\b', 0, STYLES[\"numbers\"]),\n",
      "            (r'\\b[+-]?0[xX][0-9A-Fa-f]+[lL]?\\b', 0, STYLES[\"numbers\"]),\n",
      "            (r'\\b[+-]?[0-9]+(?:\\.[0-9]+)?(?:[eE][+-]?[0-9]+)?\\b', 0, STYLES[\"numbers\"]),\n",
      "        ]\n",
      "\n",
      "        # Build a QRegExp for each pattern\n",
      "        self.rules = [(QRegExp(pat), index, fmt)\n",
      "            for (pat, index, fmt) in rules]\n",
      "\n",
      "\n",
      "    def highlightBlock(self, text):\n",
      "        \"\"\"Apply syntax highlighting to the given block of text.\n",
      "        \"\"\"\n",
      "        # Do other syntax formatting\n",
      "        for expression, nth, display_format in self.rules:\n",
      "            index = expression.indexIn(text, 0)\n",
      "\n",
      "            while index >= 0:\n",
      "                # We actually want the index of the nth match\n",
      "                index = expression.pos(nth)\n",
      "                length = expression.cap(nth).length()\n",
      "                self.setFormat(index, length, display_format)\n",
      "                index = expression.indexIn(text, index + length)\n",
      "\n",
      "        self.setCurrentBlockState(0)\n",
      "\n",
      "        # Do multi-line strings\n",
      "        in_multiline = self.match_multiline(text, *self.tri_single)\n",
      "        if not in_multiline:\n",
      "            in_multiline = self.match_multiline(text, *self.tri_double)\n",
      "\n",
      "\n",
      "    def match_multiline(self, text, delimiter, in_state, style):\n",
      "        \"\"\"Do highlighting of multi-line strings. ``delimiter`` should be a\n",
      "        ``QRegExp`` for triple-single-quotes or triple-double-quotes, and\n",
      "        ``in_state`` should be a unique integer to represent the corresponding\n",
      "        state changes when inside those strings. Returns True if we're still\n",
      "        inside a multi-line string when this function is finished.\n",
      "        \"\"\"\n",
      "        # If inside triple-single quotes, start at 0\n",
      "        if self.previousBlockState() == in_state:\n",
      "            start = 0\n",
      "            add = 0\n",
      "        # Otherwise, look for the delimiter on this line\n",
      "        else:\n",
      "            start = delimiter.indexIn(text)\n",
      "            # Move past this match\n",
      "            add = delimiter.matchedLength()\n",
      "\n",
      "        # As long as there's a delimiter match on this line...\n",
      "        while start >= 0:\n",
      "            # Look for the ending delimiter\n",
      "            end = delimiter.indexIn(text, start + add)\n",
      "            # Ending delimiter on this line?\n",
      "            if end >= add:\n",
      "                length = end - start + add + delimiter.matchedLength()\n",
      "                self.setCurrentBlockState(0)\n",
      "            # No; multi-line string\n",
      "            else:\n",
      "                self.setCurrentBlockState(in_state)\n",
      "                length = text.length() - start + add\n",
      "            # Apply formatting\n",
      "            self.setFormat(start, length, style)\n",
      "            # Look for the next match\n",
      "            start = delimiter.indexIn(text, start + length)\n",
      "\n",
      "        # Return True if still inside a multi-line string, False otherwise\n",
      "        if self.currentBlockState() == in_state:\n",
      "            return True\n",
      "        else:\n",
      "            return False\n",
      "\n",
      "def addPythonHighlighter(document):\n",
      "    PythonHighlighter(document)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search5 = \"semantic search tool for text\"\n",
    "query5 = top_n_code(search5, doc_strings, embed_vecs, 10)\n",
    "print(query5[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
