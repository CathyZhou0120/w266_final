{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import ast\n",
    "from tools import glove_helper\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "from itertools import groupby\n",
    "from os.path import basename, splitext\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the script, you will need to CMD and authenticate with \n",
    "\n",
    "'gcloud auth application-default login'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/google/auth/_default.py:66: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/.\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "client = bigquery.Client(project='manifest-frame-203601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = (\n",
    "    \"\"\"\n",
    "    select distinct repo_path,c_content from w266_final.final_20k\n",
    "    \"\"\")\n",
    "query_job = client.query(QUERY)  # API request\n",
    "rows = query_job.result()  # Waits for query to finish\n",
    "\n",
    "df = []\n",
    "for row in rows:\n",
    "    df.append([row.repo_path,row.c_content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172413, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df)\n",
    "df.columns = ['repo_path','content']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(docstring_list):\n",
    "    \n",
    "    \"\"\"takes a list of doc strings and converts to a single flat list of tokens\"\"\"\n",
    "    \n",
    "    tokens = [tf.keras.preprocessing.text.text_to_word_sequence(i) for i in docstring_list]\n",
    "    flat_tokens = [item for sublist in tokens for item in sublist]\n",
    "    flat_string = \" \".join(flat_tokens)\n",
    "    \n",
    "    return flat_string\n",
    "\n",
    "def get_docstrings(source):\n",
    "    \n",
    "    \"\"\"function to walk through parse tree and return list of docstrings\"\"\"\n",
    "    \n",
    "    NODE_TYPES = {\n",
    "    ast.ClassDef: 'Class',\n",
    "    ast.FunctionDef: 'Function/Method',\n",
    "    ast.Module: 'Module'\n",
    "    }\n",
    "    \n",
    "    docstrings = []\n",
    "    \n",
    "    try:\n",
    "        tree = ast.parse(source)\n",
    "    except:\n",
    "        return \" \"\n",
    "       \n",
    "    for node in ast.walk(tree):\n",
    "        if isinstance(node, tuple(NODE_TYPES)):\n",
    "            docstring = ast.get_docstring(node)\n",
    "            docstrings.append(docstring)\n",
    "    \n",
    "    docstrings =  [x for x in docstrings if x is not None]\n",
    "    clean_string = cleanup(docstrings)\n",
    "            \n",
    "    return clean_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docstrings'] = [get_docstrings(x) for x in list(df['content'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "hands = glove_helper.Hands(ndim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up corpus for count vectorizer\n",
    "corpus = list(df['docstrings'])\n",
    "\n",
    "#count values for tfidf calculations\n",
    "count_vect = CountVectorizer()\n",
    "count_vect = count_vect.fit(corpus)\n",
    "freq_term_matrix = count_vect.transform(corpus)\n",
    "\n",
    "#to grab columns for words\n",
    "vocab = count_vect.vocabulary_\n",
    "\n",
    "#create a holder for the new df column\n",
    "embeddings_df = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_to_embed(words):\n",
    "    \n",
    "    global count_vect, freq_term_matrix, vocab\n",
    "    \n",
    "    #verify there are docstrings available\n",
    "    if len(words)==0:\n",
    "        return np.zeros(100)\n",
    "         \n",
    "    #create tfidf for each document\n",
    "    tfidf = TfidfTransformer(norm=\"l2\")\n",
    "    tfidf.fit(freq_term_matrix)\n",
    "    doc_freq_term = count_vect.transform([words])\n",
    "    idfs = tfidf.transform(doc_freq_term)\n",
    "\n",
    "    #split the docstrings to individual words for average\n",
    "    sent_list = words.split(\" \")\n",
    "    embeddings = []\n",
    "\n",
    "    #cycle through list of words in docstring\n",
    "    for i in range(len(sent_list)):\n",
    "\n",
    "        if sent_list[i] in vocab:\n",
    "\n",
    "            col = vocab[sent_list[i]]\n",
    "            embed = hands.get_vector(sent_list[i], strict=False)\n",
    "            tfidf = idfs[0, col]\n",
    "            embeddings.append(np.multiply(embed, tfidf))\n",
    "\n",
    "        embed_array = np.asarray(embeddings)\n",
    "        \n",
    "        if len(embed_array)==0:\n",
    "            return np.zeros(100)\n",
    "\n",
    "        return np.mean(embed_array, axis=0)\n",
    "    \n",
    "def find_nn(words, embeddings):\n",
    "    \n",
    "    search = words_to_embed(words)\n",
    "    distances = [scipy.spatial.distance.cosine(search, i) for i in embeddings]\n",
    "    nn = np.argsort(np.asarray(distances))\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embeddings'] = [words_to_embed(x) for x in list(df['docstrings'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_code(search_terms, docstrings, embeddings, n):\n",
    "    \n",
    "    top_n = find_nn(search_terms, embeddings)[0:n]\n",
    "    code = [df['content'][i] for i in top_n]\n",
    "    \n",
    "    return code\n",
    "\n",
    "doc_strings = list(df['docstrings'])\n",
    "embed_vecs = list(df['embeddings'])\n",
    "\n",
    "def make_query_file(query, results, filename):\n",
    "    \n",
    "    output = open(filename, 'w')\n",
    "    for item in results:\n",
    "        output.write(\"Query: \"+query+'\\n')\n",
    "        output.write(\"\\n************************** NEXT RESULT **************************************\\n\")\n",
    "        output.write(\"%s\\n\" % item)\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "search1 = \"function that calculates distance\"\n",
    "search2 = 'merge two lists'\n",
    "search3 = 'remove duplicates from sorted array'\n",
    "search3 = 'determine if a Sudoku is valid'\n",
    "search4 = 'unique binary search tree'\n",
    "search5 = 'voice recognition function'\n",
    "search6 = 'LSTM model for semantic search'\n",
    "\n",
    "searches = [search1, search2, search3, search4, search5, search6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andrew_mamroth/anaconda3/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(searches)):\n",
    "    query = top_n_code(searches[i], doc_strings, embed_vecs, 10)\n",
    "    x=i+1\n",
    "    filename = 'model_1_queries/query'+str(x)+'.txt'\n",
    "    make_query_file(searches[i], query, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
